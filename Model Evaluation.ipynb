{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc38502",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install rouge -q\n",
    "!pip3 install evaluate -q\n",
    "!pip3 install rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7b18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "from rouge import Rouge\n",
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171909b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(t_split='train'):\n",
    "\n",
    "  # Split handling - validation set further split into 50% dev/test.\n",
    "  if t_split == 'train':\n",
    "    df = pd.DataFrame(load_dataset('squad')['train'])\n",
    "  elif t_split in ['val','test']:\n",
    "    vt_df = pd.DataFrame(load_dataset('squad')['validation'])\n",
    "    df_val = vt_df.sample(frac=0.5,random_state=266)\n",
    "    if t_split == 'test':\n",
    "      df_test = vt_df.drop(df_val.index)\n",
    "      df = df_test\n",
    "    else:\n",
    "      df = df_val\n",
    "  else:\n",
    "    raise Exception(\"Invalid choice of dataset split.\")\n",
    "  \n",
    "\n",
    "  df['answer_text'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "  df['source'] = 'answer: ' + df['answer_text'] + ' context: ' + df['context'] + '</s>'\n",
    "  df['target'] = df['question']\n",
    "\n",
    "  return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e604b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f4932d135d44329780519c28ef34c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('FINAL_reference_dict_GPP256.json', 'r') as fp:\n",
    "    reference_dict = json.load(fp)\n",
    "with open('FINAL_prediction_dict_GPP256.json', 'r') as fp:\n",
    "    prediction_dict = json.load(fp)\n",
    "    \n",
    "val_df = parse_data('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5288131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'chrf', 'code_eval', 'comet', 'competition_math', 'coval', 'cuad', 'exact_match', 'f1', 'frugalscore', 'glue', 'google_bleu', 'indic_glue', 'mae', 'mahalanobis', 'matthews_correlation', 'mauve', 'mean_iou', 'meteor', 'mse', 'pearsonr', 'perplexity', 'poseval', 'precision', 'recall', 'rl_reliability', 'roc_auc', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'ter', 'trec_eval', 'wer', 'wiki_split', 'xnli', 'xtreme_s', 'angelina-wang/directional_bias_amplification', 'codeparrot/apps_metric', 'cpllab/syntaxgym', 'daiyizheng/valid', 'erntkn/dice_coefficient', 'hack/test_metric', 'jordyvl/ece', 'kaggle/ai4code', 'kaggle/amex', 'loubnabnl/apps_metric2', 'lvwerra/bary_score', 'lvwerra/test', 'mathemakitten/harness_sentiment', 'mfumanelli/geometric_mean', 'mgfrantz/roc_auc_macro', 'yzha/ctc_eval']\n"
     ]
    }
   ],
   "source": [
    "# List possible evaluation metrics\n",
    "from datasets import list_metrics\n",
    "\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list)\n",
    "print(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78d6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_preds(reference_dict, prediction_dict, val_df):\n",
    "    '''\n",
    "    Returns the results of metrics tests on the entire prediction set and scores each individual prediction.\n",
    "    \n",
    "    The output is a pandas DataFrame with line-by-line scores and a dictionary with set scores.\n",
    "    '''\n",
    "        \n",
    "    # Load metrics\n",
    "    metrics = evaluate.combine(['bleu', 'sacrebleu', 'meteor', 'rouge'])\n",
    "    \n",
    "    # Predictions and baseline\n",
    "    refs = [reference_dict['values'][x]['target'][0] for x in range(0,len(reference_dict['values']))]\n",
    "    preds = [prediction_dict['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict['values']))]\n",
    "    answers = val_df['answer_text'].to_list()\n",
    "    context = val_df['context'].to_list()\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    scores = [metrics.compute(predictions = [preds[x]], references= [refs[x]]) for x in range(0,len(reference_dict['values']))]\n",
    "    bleus = [x['bleu'] for x in scores]\n",
    "    sacrebleus = [x['score'] for x in scores]\n",
    "    meteors = [x['meteor'] for x in scores]\n",
    "    rouges = [x['rougeL'] for x in scores]\n",
    "    \n",
    "    # Contruct dataframe with results\n",
    "    df = pd.DataFrame(list(zip(refs,preds,answers,context, bleus, sacrebleus, meteors, rouges)\n",
    "        ), columns = ['Reference', 'Prediction', 'Answer', 'Context', 'BLEU', 'SacreBLEU', 'METEOR', 'ROUGE'])\n",
    "    df['Answer_Contamination'] = df.apply(lambda x: str(x['Answer']) in str(x['Prediction']), axis=1)\n",
    "    \n",
    "    # Get results for enitre set.\n",
    "    refs = [[reference_dict['values'][x]['target'][0]] for x in range(0,len(reference_dict['values']))]\n",
    "    preds = [[prediction_dict['values'][x]['generated'].split('\\t')[0]] for x in range(0,len(prediction_dict['values']))]\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        metrics.add_batch(references=ref, predictions=pred)\n",
    "    results = metrics.compute()                          \n",
    "                       \n",
    "    return df, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238ac9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_set(reference_dict, prediction_dict, val_df):\n",
    "    '''\n",
    "    Returns the results of metrics tests on the entire prediction set.\n",
    "    \n",
    "    Does not score individual predictions.\n",
    "    '''\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics = evaluate.combine(['bleu', 'sacrebleu', 'meteor', 'rouge'])\n",
    "    \n",
    "    # Get results for enitre set.\n",
    "    refs = [[reference_dict['values'][x]['target'][0]] for x in range(0,len(reference_dict['values']))]\n",
    "    preds = [[prediction_dict['values'][x]['generated'].split('\\t')[0]] for x in range(0,len(prediction_dict['values']))]\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        metrics.add_batch(references=ref, predictions=pred)\n",
    "    results = metrics.compute()                          \n",
    "                       \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c578450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to detailed scores.\n",
    "df, results = score_preds(reference_dict, prediction_dict, val_df)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation\n",
    "df.to_pickle('df_baseline256.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f29d555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.1656587608025032,\n",
       " 'bleu_precisions': [0.4406215316315205,\n",
       "  0.19621654987110126,\n",
       "  0.11763760869128663,\n",
       "  0.07404744787922359],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0063682587313496,\n",
       " 'translation_length': 60367,\n",
       " 'reference_length': 59985,\n",
       " 'score': 16.56587608025032,\n",
       " 'counts': [26599, 10808, 5858, 3296],\n",
       " 'totals': [60367, 55082, 49797, 44512],\n",
       " 'sacrebleu_precisions': [44.062153163152054,\n",
       "  19.621654987110126,\n",
       "  11.763760869128662,\n",
       "  7.404744787922358],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 60367,\n",
       " 'ref_len': 59985,\n",
       " 'meteor': 0.4143196855920803,\n",
       " 'rouge1': 0.4356071411053506,\n",
       " 'rouge2': 0.22499535880886862,\n",
       " 'rougeL': 0.4018497255426625,\n",
       " 'rougeLsum': 0.4021833461370723}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call function to get top-level scores.\n",
    "set_results = score_set(reference_dict, prediction_dict, val_df)\n",
    "set_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reference_dict_base256.json', 'r') as fp:\n",
    "    reference_dict = json.load(fp)\n",
    "with open('prediction_dict_base256.json', 'r') as fp:\n",
    "    prediction_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30f690fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720db05ea3714f37be982c6de3559862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_GPP256.pkl {'bleu': 0.1656587608025032, 'bleu_precisions': [0.4406215316315205, 0.19621654987110126, 0.11763760869128663, 0.07404744787922359], 'brevity_penalty': 1.0, 'length_ratio': 1.0063682587313496, 'translation_length': 60367, 'reference_length': 59985, 'score': 16.56587608025032, 'counts': [26599, 10808, 5858, 3296], 'totals': [60367, 55082, 49797, 44512], 'sacrebleu_precisions': [44.062153163152054, 19.621654987110126, 11.763760869128662, 7.404744787922358], 'bp': 1.0, 'sys_len': 60367, 'ref_len': 59985, 'meteor': 0.4143196855920803, 'rouge1': 0.4356071411053506, 'rouge2': 0.22499535880886862, 'rougeL': 0.4018497255426625, 'rougeLsum': 0.4021833461370723}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_GPP256_NS.pkl {'bleu': 0.1364246031118634, 'bleu_precisions': [0.43699918088251005, 0.17553515617321566, 0.099719224357287, 0.05947448080788031], 'brevity_penalty': 0.9341231345067288, 'length_ratio': 0.9362007168458781, 'translation_length': 56158, 'reference_length': 59985, 'score': 13.64246031118634, 'counts': [24541, 8930, 4546, 2397], 'totals': [56158, 50873, 45588, 40303], 'sacrebleu_precisions': [43.699918088251, 17.553515617321565, 9.9719224357287, 5.947448080788031], 'bp': 0.9341231345067288, 'sys_len': 56158, 'ref_len': 59985, 'meteor': 0.37423245714266146, 'rouge1': 0.40529660894059427, 'rouge2': 0.19167726547018954, 'rougeL': 0.3732356714376023, 'rougeLsum': 0.3735456607916215}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on multiple prediction sets and save the results.\n",
    "test_df = parse_data('test')\n",
    "pred_sets = [\n",
    "    'FINAL_prediction_dict_GPP256.json',\n",
    "    'FINAL_NS_prediction_dict_GPP256.json'\n",
    "]\n",
    "ref_sets = [\n",
    "    'FINAL_reference_dict_GPP256.json',  \n",
    "    'FINAL_NS_reference_dict_GPP256.json'\n",
    "]\n",
    "save_names = [\n",
    "    'df_GPP256.pkl',\n",
    "    'df_GPP256_NS.pkl'\n",
    "]\n",
    "\n",
    "for x in range(0,len(pred_sets)):\n",
    "    with open(ref_sets[x], 'r') as fp:\n",
    "        reference_dict = json.load(fp)\n",
    "    with open(pred_sets[x], 'r') as fp:\n",
    "        prediction_dict = json.load(fp)\n",
    "        \n",
    "    df, results = score_preds(reference_dict, prediction_dict, test_df)\n",
    "    print(save_names[x], results)\n",
    "    \n",
    "    df.to_pickle(save_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69961154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010703bb2a1d4b5fa3123447383ef6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bleu</th>\n",
       "      <th>sacrebleu</th>\n",
       "      <th>meteor</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline (BS)</td>\n",
       "      <td>0.211274</td>\n",
       "      <td>21.127372</td>\n",
       "      <td>0.474482</td>\n",
       "      <td>0.462751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline (NS)</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>18.258757</td>\n",
       "      <td>0.439014</td>\n",
       "      <td>0.442596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPP (BS)</td>\n",
       "      <td>0.165659</td>\n",
       "      <td>16.565876</td>\n",
       "      <td>0.414320</td>\n",
       "      <td>0.401850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPP (NS)</td>\n",
       "      <td>0.136425</td>\n",
       "      <td>13.642460</td>\n",
       "      <td>0.374232</td>\n",
       "      <td>0.373236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model      bleu  sacrebleu    meteor    rougeL\n",
       "0  Baseline (BS)  0.211274  21.127372  0.474482  0.462751\n",
       "1  Baseline (NS)  0.182588  18.258757  0.439014  0.442596\n",
       "2       GPP (BS)  0.165659  16.565876  0.414320  0.401850\n",
       "3       GPP (NS)  0.136425  13.642460  0.374232  0.373236"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evaluation on multiple prediction sets and save the results.\n",
    "test_df = parse_data('test')\n",
    "pred_sets = [\n",
    "    'FINAL_prediction_dict_base256.json',\n",
    "    'FINAL_NS_prediction_dict_base256.json',\n",
    "    'FINAL_prediction_dict_GPP256.json',\n",
    "    'FINAL_NS_prediction_dict_GPP256.json'\n",
    "]\n",
    "ref_sets = [\n",
    "    'FINAL_reference_dict_base256.json',  \n",
    "    'FINAL_NS_reference_dict_base256.json',\n",
    "    'FINAL_reference_dict_GPP256.json',  \n",
    "    'FINAL_NS_reference_dict_GPP256.json'\n",
    "]\n",
    "\n",
    "model_grades = []\n",
    "\n",
    "for x in range(0,len(pred_sets)):\n",
    "    with open(ref_sets[x], 'r') as fp:\n",
    "        reference_dict = json.load(fp)\n",
    "    with open(pred_sets[x], 'r') as fp:\n",
    "        prediction_dict = json.load(fp)\n",
    "        \n",
    "    model_grades.append(score_set(reference_dict, prediction_dict, test_df))\n",
    "    \n",
    "model_grades\n",
    "grades_df = pd.DataFrame(model_grades)\n",
    "grades_df['model'] = ['Baseline (BS)', 'Baseline (NS)', 'GPP (BS)', 'GPP (NS)']\n",
    "grades_df[['model','bleu','score','meteor','rougeL']].rename(columns = {'score': 'sacrebleu'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
