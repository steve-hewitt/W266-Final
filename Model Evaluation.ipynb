{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fc38502",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install rouge -q\n",
    "!pip3 install evaluate -q\n",
    "!pip3 install rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8d7b18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "from rouge import Rouge\n",
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "171909b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(t_split='train'):\n",
    "\n",
    "  # Split handling - validation set further split into 50% dev/test.\n",
    "  if t_split == 'train':\n",
    "    df = pd.DataFrame(load_dataset('squad')['train'])\n",
    "  elif t_split in ['val','test']:\n",
    "    vt_df = pd.DataFrame(load_dataset('squad')['validation'])\n",
    "    df_val = vt_df.sample(frac=0.5,random_state=266)\n",
    "    if t_split == 'test':\n",
    "      df_test = vt_df.drop(df_val.index)\n",
    "      df = df_test\n",
    "    else:\n",
    "      df = df_val\n",
    "  else:\n",
    "    raise Exception(\"Invalid choice of dataset split.\")\n",
    "  \n",
    "\n",
    "  df['answer_text'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "  df['source'] = 'answer: ' + df['answer_text'] + ' context: ' + df['context'] + '</s>'\n",
    "  df['target'] = df['question']\n",
    "\n",
    "  return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e604b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2077e988fc4e7796fd55f49f2f9530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('reference_dict_base256.json', 'r') as fp:\n",
    "    reference_dict = json.load(fp)\n",
    "with open('prediction_dict_base256.json', 'r') as fp:\n",
    "    prediction_dict = json.load(fp)\n",
    "    \n",
    "val_df = parse_data('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e5288131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'chrf', 'code_eval', 'comet', 'competition_math', 'coval', 'cuad', 'exact_match', 'f1', 'frugalscore', 'glue', 'google_bleu', 'indic_glue', 'mae', 'mahalanobis', 'matthews_correlation', 'mauve', 'mean_iou', 'meteor', 'mse', 'pearsonr', 'perplexity', 'poseval', 'precision', 'recall', 'rl_reliability', 'roc_auc', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'ter', 'trec_eval', 'wer', 'wiki_split', 'xnli', 'xtreme_s', 'angelina-wang/directional_bias_amplification', 'codeparrot/apps_metric', 'cpllab/syntaxgym', 'daiyizheng/valid', 'erntkn/dice_coefficient', 'hack/test_metric', 'jordyvl/ece', 'kaggle/ai4code', 'kaggle/amex', 'loubnabnl/apps_metric2', 'lvwerra/bary_score', 'lvwerra/test', 'mfumanelli/geometric_mean', 'mgfrantz/roc_auc_macro', 'yzha/ctc_eval']\n"
     ]
    }
   ],
   "source": [
    "# List possible evaluation metrics\n",
    "from datasets import list_metrics\n",
    "\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list)\n",
    "print(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e78d6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_preds(reference_dict, prediction_dict, val_df):\n",
    "    '''\n",
    "    Returns the results of metrics tests on the entire prediction set and scores each individual prediction.\n",
    "    \n",
    "    The output is a pandas DataFrame with line-by-line scores and a dictionary with set scores.\n",
    "    '''\n",
    "        \n",
    "    # Load metrics\n",
    "    metrics = evaluate.combine(['bleu', 'sacrebleu', 'meteor', 'rouge'])\n",
    "    \n",
    "    # Predictions and baseline\n",
    "    refs = [reference_dict['values'][x]['target'][0] for x in range(0,len(reference_dict['values']))]\n",
    "    preds = [prediction_dict['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict['values']))]\n",
    "    answers = val_df['answer_text'].to_list()\n",
    "    context = val_df['context'].to_list()\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    scores = [metrics.compute(predictions = [preds[x]], references= [refs[x]]) for x in range(0,len(reference_dict['values']))]\n",
    "    bleus = [x['bleu'] for x in scores]\n",
    "    sacrebleus = [x['score'] for x in scores]\n",
    "    meteors = [x['meteor'] for x in scores]\n",
    "    rouges = [x['rougeL'] for x in scores]\n",
    "    \n",
    "    # Contruct dataframe with results\n",
    "    df = pd.DataFrame(list(zip(refs,preds,answers,context, bleus, sacrebleus, meteors, rouges)\n",
    "        ), columns = ['Reference', 'Prediction', 'Answer', 'Context', 'BLEU', 'SacreBLEU', 'METEOR', 'ROUGE'])\n",
    "    df['Answer_Contamination'] = df.apply(lambda x: str(x['Answer']) in str(x['Prediction']), axis=1)\n",
    "    \n",
    "    # Get results for enitre set.\n",
    "    refs = [[reference_dict['values'][x]['target'][0]] for x in range(0,len(reference_dict['values']))]\n",
    "    preds = [[prediction_dict['values'][x]['generated'].split('\\t')[0]] for x in range(0,len(prediction_dict['values']))]\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        metrics.add_batch(references=ref, predictions=pred)\n",
    "    results = metrics.compute()                          \n",
    "                       \n",
    "    return df, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "238ac9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_set(reference_dict, prediction_dict, val_df):\n",
    "    '''\n",
    "    Returns the results of metrics tests on the entire prediction set.\n",
    "    \n",
    "    Does not score individual predictions.\n",
    "    '''\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics = evaluate.combine(['bleu', 'sacrebleu', 'meteor', 'rouge'])\n",
    "    \n",
    "    # Get results for enitre set.\n",
    "    refs = [[reference_dict['values'][x]['target'][0]] for x in range(0,len(reference_dict['values']))]\n",
    "    preds = [[prediction_dict['values'][x]['generated'].split('\\t')[0]] for x in range(0,len(prediction_dict['values']))]\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        metrics.add_batch(references=ref, predictions=pred)\n",
    "    results = metrics.compute()                          \n",
    "                       \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c578450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to detailed scores.\n",
    "df, results = score_preds(reference_dict, prediction_dict, val_df)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation\n",
    "df.to_pickle('df_baseline256.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to get top-level scores.\n",
    "set_results = score_set(reference_dict, prediction_dict, val_df)\n",
    "set_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reference_dict_base256.json', 'r') as fp:\n",
    "    reference_dict = json.load(fp)\n",
    "with open('prediction_dict_base256.json', 'r') as fp:\n",
    "    prediction_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f690fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b70a3d3f40b4130b3f4bfe27c4c0694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on multiple prediction sets and save the results.\n",
    "val_df = parse_data('val')\n",
    "pred_sets = [\n",
    "    'prediction_dict_base64.json',\n",
    "    'prediction_dict_base128.json',\n",
    "    'prediction_dict_base512.json',\n",
    "    'prediction_dict_GPP256.json'\n",
    "]\n",
    "ref_sets = [\n",
    "    'reference_dict_base64.json',\n",
    "    'reference_dict_base128.json',\n",
    "    'reference_dict_base512.json',\n",
    "    'reference_dict_GPP256.json'  \n",
    "]\n",
    "save_names = [\n",
    "    'df_baseline64.pkl'\n",
    "    'df_baseline128.pkl'\n",
    "    'df_baseline516.pkl'\n",
    "    'df_GPP256_original.pkl'\n",
    "]\n",
    "\n",
    "for x in range(0,len(pred_sets)):\n",
    "    with open(ref_sets[x], 'r') as fp:\n",
    "        reference_dict = json.load(fp)\n",
    "    with open(pred_sets[x], 'r') as fp:\n",
    "        prediction_dict = json.load(fp)\n",
    "        \n",
    "    df, results = score_preds(reference_dict, prediction_dict, val_df)\n",
    "    print(save_names[x], results)\n",
    "    \n",
    "    df.to_pickle(save_names[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
