{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e73d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from packaging import version\n",
    "\n",
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "from datasets import Dataset\n",
    "\n",
    "import sacrebleu\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ee1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions from GP-VAE implementation\n",
    "\n",
    "#def parse_data(in_file='../../data/GYAFC/em/trn.tsv'):\n",
    "#    with open(in_file, 'r') as f:\n",
    "#        data = f.read().split('\\n')\n",
    "#        data.remove('')\n",
    "#    contexted = []\n",
    "#    for i, line in enumerate(data):\n",
    "#        source_txt = line.split('\\t')[0]\n",
    "#        target_txt = line.split('\\t')[1]\n",
    "#        row = (i, source_txt, target_txt)\n",
    "#        contexted.append(row)\n",
    "#    columns = ['id', 'source', 'target']\n",
    "#    data_df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "#    return data_df\n",
    "\n",
    "# Specific to dataset.\n",
    "def construct_input_for_batch(tokenizer, batch, args):\n",
    "    \"\"\"\n",
    "    Function that takes a batch from a dataset and constructs the corresponding \n",
    "    input string.\n",
    "    \"\"\"\n",
    "    source, target = [], []\n",
    "    for inp, out in zip(batch['source'], batch['target']):\n",
    "        source.append(inp.strip())\n",
    "        target.append(out.strip())\n",
    "    if batch['id'][0] == 0:\n",
    "        print(source[0])\n",
    "        print(target[0])\n",
    "        print()\n",
    "    return source, target\n",
    "\n",
    "def make_batch_inputs(batch, tokenizer, args, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Function that takes a batch from a dataset and transforms it \n",
    "  \"\"\"\n",
    "  # Concatenate the concept names for each example in the batch.\n",
    "  input_lists, _ = construct_input_for_batch(tokenizer, batch, args)\n",
    "  # Use the model's tokenizer to create the batch input_ids.\n",
    "  batch_features = tokenizer(input_lists, padding=True, return_tensors='pt')\n",
    "  # Move all inputs to the device.\n",
    "  batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
    "  return batch_features\n",
    "\n",
    "def make_batch_data(batch, tokenizer, args, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Function that takes a batch from a dataset and transforms it \n",
    "  \"\"\"\n",
    "  # Concatenate the concept names for each example in the batch.\n",
    "  input_lists, label_list = construct_input_for_batch(tokenizer, batch, args)\n",
    "  # Use the model's tokenizer to create the batch input_ids.\n",
    "  batch_features = tokenizer(input_lists, padding=True, return_tensors='pt')\n",
    "  batch_labels = tokenizer(label_list, padding=True, return_tensors='pt')\n",
    "  # Move all inputs to the device.\n",
    "  batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
    "  batch_labels = dict([(k, v.to(device)) for k, v in batch_labels.items()])\n",
    "  return batch_features, batch_labels\n",
    "\n",
    "def batch_tokenize(dataset_batch, tokenizer, args):\n",
    "  \"\"\"\n",
    "  Reuse the function defined above to construct the batch (source, target) and \n",
    "  run them through the tokenizer.\n",
    "  \"\"\"\n",
    "  source, target = construct_input_for_batch(tokenizer, dataset_batch, args)\n",
    "  res = {\n",
    "          \"input_ids\": tokenizer(\n",
    "              source,\n",
    "              padding='max_length', \n",
    "              truncation=True,\n",
    "              max_length=args.encoder_max_length\n",
    "          )[\"input_ids\"],\n",
    "          \"labels\": tokenizer(\n",
    "              target,\n",
    "              padding='max_length', \n",
    "              truncation=True,\n",
    "              max_length=args.decoder_max_length\n",
    "          )[\"input_ids\"],\n",
    "  }\n",
    "  return res\n",
    "\n",
    "def batchify_data(df, tokenizer, args):\n",
    "  dataset = Dataset.from_pandas(df)\n",
    "  data_tokenized = dataset.map(\n",
    "    lambda batch: batch_tokenize(batch, tokenizer, args),\n",
    "    batched=True\n",
    "  )\n",
    "  return data_tokenized\n",
    "\n",
    "def compute_loss(batch, model, tokenizer, args):\n",
    "  batch_feature, batch_label = make_batch_data(batch, tokenizer, args)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids=batch_feature['input_ids'],\n",
    "                    labels=batch_label['input_ids'])\n",
    "    eval_loss = outputs.loss.item()\n",
    "  return [eval_loss] \n",
    "\n",
    "def test_ppl(val_df, model, tokenizer, args):\n",
    "  loss_dict = Dataset.from_pandas(val_df).map(\n",
    "    lambda batch: {'loss': compute_loss(batch, model, tokenizer, args)},\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "  )\n",
    "  \n",
    "  eval_loss = 0.\n",
    "  nb_eval_steps = 0\n",
    "  for item in list(loss_dict):\n",
    "      eval_loss += item['loss']\n",
    "      nb_eval_steps += 1\n",
    "  eval_loss = eval_loss / nb_eval_steps\n",
    "  ppl = torch.exp(torch.tensor(eval_loss))\n",
    "  return ppl.item()\n",
    "\n",
    "def prepare_eval(output_list):\n",
    "    ref_list, pred_list = [], []\n",
    "    for item in output_list:\n",
    "        pred_list.append({\"generated\": item['generated']})\n",
    "        ref_list.append({\"target\": [item['target']]})\n",
    "    return ref_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defb4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing dataset constructing function from utilities with a custom one.\n",
    "def parse_data(t_split='train'):\n",
    "  # Get dataset.\n",
    "  #squad_dataset = load_dataset('squad') # Method A - standard version\n",
    "  #squad_dataset, info = tfds.load('squad_question_generation/split_zhou', with_info=True, split='train') # Method B - version with split that could be better.\n",
    "\n",
    "  # Split handling - validation set further split into 50% dev/test.\n",
    "  if t_split == 'train':\n",
    "    df = pd.DataFrame(load_dataset('squad')['train'])\n",
    "  elif t_split in ['val','test']:\n",
    "    vt_df = pd.DataFrame(load_dataset('squad')['validation'])\n",
    "    df_val = vt_df.sample(frac=0.5,random_state=266)\n",
    "    if t_split == 'test':\n",
    "      df_test = vt_df.drop(df_val.index)\n",
    "      df = df_test\n",
    "    else:\n",
    "      df = df_val\n",
    "  else:\n",
    "    raise Exception(\"Invalid choice of dataset split.\")\n",
    "  \n",
    "\n",
    "  df['answer_text'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "  df['source'] = 'answer: ' + df['answer_text'] + ' context: ' + df['context'] + '</s>'\n",
    "  df['target'] = df['question']\n",
    "\n",
    "  return df                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ce922db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transformers.file_utils import is_apex_available\n",
    "\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "class Seq2SeqTrainer(Trainer):\n",
    "    \"\"\"Class to finetune a Seq2Seq model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_beams=4,\n",
    "            max_length=32,\n",
    "            *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_beams = num_beams\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        outputs = model(input_ids=inputs['input_ids'],\n",
    "                        # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                        labels=inputs['labels'])\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            return self.label_smoother(outputs, inputs[\"labels\"])\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            return outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        Runs the model to either generate a sequence and/or compute the loss.\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        # Compute loss with labels first.\n",
    "        with torch.no_grad():\n",
    "            if self.args.fp16 and _use_native_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids=inputs['input_ids'],\n",
    "                                    # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                                    labels=inputs['labels'])\n",
    "            else:\n",
    "                outputs = model(input_ids=inputs['input_ids'],\n",
    "                                # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                                labels=inputs['labels'])\n",
    "            if has_labels:\n",
    "                loss = outputs[0].mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "        # If we're only computing the conditional log-likelihood, return.\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        # Otherwise run model.generate() to get predictions.\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            preds = model.module.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        else:\n",
    "            preds = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        if len(preds) == 1:\n",
    "            preds = preds[0]\n",
    "        # Pad predictions if necessary so they can be concatenated across batches.\n",
    "        if preds.shape[-1] < self.max_length:\n",
    "            preds = torch.nn.functional.pad(\n",
    "                preds, (0, self.max_length - preds.shape[-1]),\n",
    "                mode='constant',\n",
    "                value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        # Post-process labels.\n",
    "        if has_labels:\n",
    "            labels = inputs.get('labels')\n",
    "        else:\n",
    "            labels = None\n",
    "        return (loss, preds, labels)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Load the dataset\n",
    "    #trn_df = parse_data(in_file=f'../../data/{args.dataset}/trn.tsv')\n",
    "    #val_df = parse_data(in_file=f'../../data/{args.dataset}/val.tsv')\n",
    "    trn_df = parse_data('train')\n",
    "    val_df = parse_data('val')\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    ckpt_path = None\n",
    "    if args.task == 'train':\n",
    "        ckpt_path = args.model_name\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        # update timestamp and create new path for ckpt\n",
    "        args.timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    train_data_tokenized = batchify_data(trn_df, tokenizer, args)\n",
    "    valid_data_tokenized = batchify_data(val_df, tokenizer, args)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "    model = model.to('cuda:0')\n",
    "\n",
    "    # Training Setup\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=300,\n",
    "        logging_steps=100,\n",
    "        # optimization args, the trainer uses the Adam optimizer\n",
    "        # and has a linear warmup for the learning rate\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-04,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=0,\n",
    "        lr_scheduler_type='constant',\n",
    "        # misc args\n",
    "        seed=42,\n",
    "        save_total_limit=5,  # limit the total amount of checkpoints\n",
    "        disable_tqdm=False,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,\n",
    "        local_rank=args.local_rank\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        num_beams=args.beam_size,\n",
    "        max_length=args.decoder_max_length,\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_data_tokenized,\n",
    "        eval_dataset=valid_data_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Now that we have the trainer set up, we can finetune.\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def beam_generate_sentences(batch,\n",
    "                            model,\n",
    "                            tokenizer,\n",
    "                            args,\n",
    "                            device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "    # Generate with beam search.\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        num_beams=args.beam_size,\n",
    "        max_length=args.max_generation_length,\n",
    "        num_return_sequences=args.num_return_sequences,\n",
    "    )\n",
    "    # Use model tokenizer to decode to text.\n",
    "    generated_sentences = [\n",
    "        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "        for gen_ids in generated_ids\n",
    "    ]\n",
    "    # print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "\n",
    "def sample_sentences(batch,\n",
    "                     model,\n",
    "                     tokenizer,\n",
    "                     args,\n",
    "                     device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "\n",
    "    generated_sentences = []\n",
    "    for i in range(args.num_return_sequences):\n",
    "        # Generate with beam search.\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=features['input_ids'],\n",
    "            attention_mask=features['attention_mask'],\n",
    "            num_beams=args.beam_size,\n",
    "            max_length=args.max_generation_length,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        # Use model tokenizer to decode to text.\n",
    "        generated_sentences += [\n",
    "            tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "            for gen_ids in generated_ids\n",
    "        ]\n",
    "    # print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    te_df = parse_data('test')\n",
    "    print('Data loaded!!!')\n",
    "\n",
    "    # Load the model\n",
    "    if args.timestamp == '0':\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(f\"{args.model_name}\")\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    if args.timestamp == '0':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(f\"{args.model_name}\")\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        model = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "    model = model.to('cuda:0')\n",
    "    model.kernel_v = args.kernel_v\n",
    "    model.kernel_r = args.kernel_r\n",
    "    model.from_mean = args.from_mean\n",
    "    model.scaler = args.scaler\n",
    "\n",
    "    # Make predictions\n",
    "    if args.from_mean:\n",
    "        test_output = Dataset.from_pandas(te_df).map(\n",
    "            lambda batch: {'generated': beam_generate_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "    else:\n",
    "        test_output = Dataset.from_pandas(te_df).map(\n",
    "            lambda batch: {'generated': sample_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "    # prepare evaluation data\n",
    "    ref_list, pred_list = prepare_eval(list(test_output))\n",
    "    reference_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": ref_list,\n",
    "    }\n",
    "    prediction_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": pred_list,\n",
    "    }\n",
    "\n",
    "    if args.timestamp == '0':\n",
    "        os.makedirs(f\"{args.model_name}_{args.dataset}_{args.flag}_{args.timestamp}\")\n",
    "\n",
    "    with open(\n",
    "            f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/refs.json\",\n",
    "            'w') as f:\n",
    "        f.write(json.dumps(reference_dict, indent=2))\n",
    "    if args.from_mean:\n",
    "        with open(\n",
    "                f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/outs_mean.json\",\n",
    "                'w') as f:\n",
    "            f.write(json.dumps(prediction_dict, indent=2))\n",
    "    else:\n",
    "        with open(\n",
    "                f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/outs.json\",\n",
    "                'w') as f:\n",
    "            f.write(json.dumps(prediction_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8320e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = argparse.ArgumentParser(description='Hyperparams')\n",
    "p.add_argument('-t', '--task', type=str, default=\"train\",\n",
    "                help=\"specify the task to do: (train)ing, ft(finetune), (eval)uation\")\n",
    "#p.add_argument('-t', '--task', type=str, default=\"ft\",\n",
    "#                help=\"specify the task to do: (train)ing, ft(finetune), (eval)uation\")\n",
    "p.add_argument('-c', '--ckpt', type=str, default=\"193280\",\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-time', '--timestamp', type=str, default='2021-02-14-04-57-04',\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-f', '--flag', type=str, default='gpvae',\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-d', '--dataset', type=str, default=\"GYAFC/em\",\n",
    "                help=\"specify the dataset: GYAFC/em, GYAFC/fr\")\n",
    "p.add_argument('--model_name', type=str, default=\"t5-base\",\n",
    "                help=\"specify the model name: t5-base, facebook/blenderbot-400M-distill\")\n",
    "p.add_argument('-v', '--kernel_v', type=float, default=64.0,\n",
    "                help=\"Hyper-parameter for prior kernel,  control the signal variance\")\n",
    "p.add_argument('-r', '--kernel_r', type=float, default=0.0001,\n",
    "                help=\"Hyper-parameter for prior kernel.\")\n",
    "p.add_argument('-s', '--scaler', type=float, default=1.0)\n",
    "p.add_argument('--from_mean', action='store_true',\n",
    "                help=\"specify whether sample from mean during generation\")\n",
    "#p.add_argument('-bz', '--batch_size', type=int, default=16)\n",
    "p.add_argument('-bz', '--batch_size', type=int, default=16)\n",
    "p.add_argument('-e', '--epochs', type=int, default=10)\n",
    "#p.add_argument('--encoder_max_length', type=int, default=50)\n",
    "p.add_argument('--encoder_max_length', type=int, default=128)\n",
    "p.add_argument('--decoder_max_length', type=int, default=48)\n",
    "#p.add_argument('--max_generation_length', type=int, default=60)\n",
    "p.add_argument('--max_generation_length', type=int, default=96)\n",
    "#p.add_argument('--beam_size', type=int, default=10)\n",
    "p.add_argument('--beam_size', type=int, default=5)\n",
    "#p.add_argument('--num_return_sequences', type=int, default=10)\n",
    "p.add_argument('--num_return_sequences', type=int, default=5)\n",
    "p.add_argument('--local_rank', type=int, default=-1,\n",
    "                help=\"Multiple GPU training\")\n",
    "args = p.parse_args()\n",
    "\n",
    "# jupyter fix for bad flag\n",
    "args.flag = 't5base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62d461",
   "metadata": {},
   "source": [
    "### Generate predictions on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51ce139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd64de988d947128866b85550957db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function batchify_data.<locals>.<lambda> at 0x7fede6564ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff30f19dd4547abb13acfe4e6479cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get tokenizer, model, and dev set.\n",
    "ckpt_path = f\"t5-base_GYAFC/em_gpvae_64.0_0.0001_2022-07-12-02-30-44/checkpoint-10800\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "val_df = parse_data('val')\n",
    "valid_data_tokenized = batchify_data(val_df, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204d7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other steps\n",
    "model = model.to('cuda:0')\n",
    "model.kernel_v = args.kernel_v\n",
    "model.kernel_r = args.kernel_r\n",
    "model.from_mean = args.from_mean\n",
    "model.scaler = args.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef2e643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8540f42b32b0402baffb4bd384cbdbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5285 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "if args.from_mean:\n",
    "    test_output = Dataset.from_pandas(val_df).map(\n",
    "        lambda batch: {'generated': beam_generate_sentences(\n",
    "            batch,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            args,\n",
    "            device='cuda:0')\n",
    "        },\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "else:\n",
    "    test_output = Dataset.from_pandas(val_df).map(\n",
    "        lambda batch: {'generated': sample_sentences(\n",
    "            batch,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            args,\n",
    "            device='cuda:0')\n",
    "        },\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "# prepare evaluation data\n",
    "ref_list, pred_list = prepare_eval(list(test_output))\n",
    "reference_dict = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": ref_list,\n",
    "}\n",
    "prediction_dict = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": pred_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec31ff1",
   "metadata": {},
   "source": [
    "### Score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94efee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 23.04112437769011,\n",
       " 'counts': [31215, 14815, 8782, 5408],\n",
       " 'totals': [61093, 55808, 50523, 45238],\n",
       " 'precisions': [51.094233381893176,\n",
       "  26.546373279816514,\n",
       "  17.382182372384854,\n",
       "  11.95455148326628],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 61093,\n",
       " 'ref_len': 60122}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate BLEU-4.\n",
    "metric = datasets.load_metric('sacrebleu')\n",
    "for model_predictions, gold_references in zip(fin_preds,fin_targets):\n",
    "    metric.add(predictions=model_predictions, references=gold_references)\n",
    "final_score = metric.compute()\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9a9cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate BLEU-4 - method 2\n",
    "#fin_targets = [reference_dict['values'][x]['target'] for x in range(0,len(reference_dict['values']))]\n",
    "#fin_preds = [prediction_dict['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict['values']))]\n",
    "#bleu = sacrebleu.corpus_bleu(fin_preds, fin_targets)\n",
    "#print(bleu.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad3a96",
   "metadata": {},
   "source": [
    "### Save or Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56ab42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pred/target lists.\n",
    "with open('reference_dict.json', 'w') as fp:\n",
    "    json.dump(reference_dict, fp)\n",
    "with open('prediction_dict.json', 'w') as fp:\n",
    "    json.dump(prediction_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2944e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pred/target lists.\n",
    "with open('reference_dict.json', 'r') as fp:\n",
    "    reference_dict = json.load(fp)\n",
    "with open('prediction_dict.json', 'r') as fp:\n",
    "    prediction_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fe973",
   "metadata": {},
   "source": [
    "### Beam experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ff70c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cf2c7ffd9c49c28bf8fe4a6160f778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "if 0 == 0: #args.from_mean:\n",
    "    test_output = Dataset.from_pandas(val_df[0:5]).map(\n",
    "        lambda batch: {'generated': beam_generate_sentences(\n",
    "            batch,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            args,\n",
    "            device='cuda:0')\n",
    "        },\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "else:\n",
    "    test_output = Dataset.from_pandas(val_df[0:5]).map(\n",
    "        lambda batch: {'generated': sample_sentences(\n",
    "            batch,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            args,\n",
    "            device='cuda:0')\n",
    "        },\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "# prepare evaluation data\n",
    "ref_list, pred_list = prepare_eval(list(test_output))\n",
    "reference_dict_test = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": ref_list,\n",
    "}\n",
    "prediction_dict_test = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": pred_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56fc7cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9318     answer: 1421 context: Before the St. Elizabeth...\n",
       "10432    answer: applied force context: Pushing against...\n",
       "3106     answer: Huguon context: In this last connectio...\n",
       "5685     answer: Serge Chermayeff context: One of the e...\n",
       "7684     answer: the Museum of the Moving Image context...\n",
       "Name: source, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[0:5].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e9a2744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': ['What year did the flood that impacted the Meuse take place?']},\n",
       " {'target': ['What makes static friction go up or down in responce to contact characteristics between an object and the surface it is on?']},\n",
       " {'target': ['By what other name was the Gate known?']},\n",
       " {'target': ['A rug by which Russian-born British designer is included in the V&A collection?']},\n",
       " {'target': ['Who put on a Doctor Who exhibition in 1991?']}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_dict_test['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d22e95d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What museum in London named their exhibition \"Behind the Sofa\"?',\n",
       " 'Which museum in London named their exhibition \"Behind the Sofa\"?',\n",
       " 'What museum named their exhibition \"Behind the Sofa\"?',\n",
       " 'Who named their exhibition \"Behind the Sofa\"?',\n",
       " 'What museum in London named their exhibition \"Behind the Sofa\" in 1991?']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict_test['values'][4]['generated'].split('\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
