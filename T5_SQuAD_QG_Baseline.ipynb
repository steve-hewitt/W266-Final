{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d79f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from packaging import version\n",
    "\n",
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab31af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions from GP-VAE implementation\n",
    "\n",
    "#def parse_data(in_file='../../data/GYAFC/em/trn.tsv'):\n",
    "#    with open(in_file, 'r') as f:\n",
    "#        data = f.read().split('\\n')\n",
    "#        data.remove('')\n",
    "#    contexted = []\n",
    "#    for i, line in enumerate(data):\n",
    "#        source_txt = line.split('\\t')[0]\n",
    "#        target_txt = line.split('\\t')[1]\n",
    "#        row = (i, source_txt, target_txt)\n",
    "#        contexted.append(row)\n",
    "#    columns = ['id', 'source', 'target']\n",
    "#    data_df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "#    return data_df\n",
    "\n",
    "# Specific to dataset.\n",
    "def construct_input_for_batch(tokenizer, batch, args):\n",
    "    \"\"\"\n",
    "    Function that takes a batch from a dataset and constructs the corresponding \n",
    "    input string.\n",
    "    \"\"\"\n",
    "    source, target = [], []\n",
    "    for inp, out in zip(batch['source'], batch['target']):\n",
    "        source.append(inp.strip())\n",
    "        target.append(out.strip())\n",
    "    if batch['id'][0] == 0:\n",
    "        print(source[0])\n",
    "        print(target[0])\n",
    "        print()\n",
    "    return source, target\n",
    "\n",
    "def make_batch_inputs(batch, tokenizer, args, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Function that takes a batch from a dataset and transforms it \n",
    "  \"\"\"\n",
    "  # Concatenate the concept names for each example in the batch.\n",
    "  input_lists, _ = construct_input_for_batch(tokenizer, batch, args)\n",
    "  # Use the model's tokenizer to create the batch input_ids.\n",
    "  batch_features = tokenizer(input_lists, padding=True, return_tensors='pt')\n",
    "  # Move all inputs to the device.\n",
    "  batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
    "  return batch_features\n",
    "\n",
    "def make_batch_data(batch, tokenizer, args, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Function that takes a batch from a dataset and transforms it \n",
    "  \"\"\"\n",
    "  # Concatenate the concept names for each example in the batch.\n",
    "  input_lists, label_list = construct_input_for_batch(tokenizer, batch, args)\n",
    "  # Use the model's tokenizer to create the batch input_ids.\n",
    "  batch_features = tokenizer(input_lists, padding=True, return_tensors='pt')\n",
    "  batch_labels = tokenizer(label_list, padding=True, return_tensors='pt')\n",
    "  # Move all inputs to the device.\n",
    "  batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
    "  batch_labels = dict([(k, v.to(device)) for k, v in batch_labels.items()])\n",
    "  return batch_features, batch_labels\n",
    "\n",
    "def batch_tokenize(dataset_batch, tokenizer, args):\n",
    "  \"\"\"\n",
    "  Reuse the function defined above to construct the batch (source, target) and \n",
    "  run them through the tokenizer.\n",
    "  \"\"\"\n",
    "  source, target = construct_input_for_batch(tokenizer, dataset_batch, args)\n",
    "  res = {\n",
    "          \"input_ids\": tokenizer(\n",
    "              source,\n",
    "              padding='max_length', \n",
    "              truncation=True,\n",
    "              max_length=args.encoder_max_length\n",
    "          )[\"input_ids\"],\n",
    "          \"labels\": tokenizer(\n",
    "              target,\n",
    "              padding='max_length', \n",
    "              truncation=True,\n",
    "              max_length=args.decoder_max_length\n",
    "          )[\"input_ids\"],\n",
    "  }\n",
    "  return res\n",
    "\n",
    "def batchify_data(df, tokenizer, args):\n",
    "  dataset = Dataset.from_pandas(df)\n",
    "  data_tokenized = dataset.map(\n",
    "    lambda batch: batch_tokenize(batch, tokenizer, args),\n",
    "    batched=True\n",
    "  )\n",
    "  return data_tokenized\n",
    "\n",
    "def compute_loss(batch, model, tokenizer, args):\n",
    "  batch_feature, batch_label = make_batch_data(batch, tokenizer, args)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids=batch_feature['input_ids'],\n",
    "                    labels=batch_label['input_ids'])\n",
    "    eval_loss = outputs.loss.item()\n",
    "  return [eval_loss] \n",
    "\n",
    "def test_ppl(val_df, model, tokenizer, args):\n",
    "  loss_dict = Dataset.from_pandas(val_df).map(\n",
    "    lambda batch: {'loss': compute_loss(batch, model, tokenizer, args)},\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "  )\n",
    "  \n",
    "  eval_loss = 0.\n",
    "  nb_eval_steps = 0\n",
    "  for item in list(loss_dict):\n",
    "      eval_loss += item['loss']\n",
    "      nb_eval_steps += 1\n",
    "  eval_loss = eval_loss / nb_eval_steps\n",
    "  ppl = torch.exp(torch.tensor(eval_loss))\n",
    "  return ppl.item()\n",
    "\n",
    "def prepare_eval(output_list):\n",
    "    ref_list, pred_list = [], []\n",
    "    for item in output_list:\n",
    "        pred_list.append({\"generated\": item['generated']})\n",
    "        ref_list.append({\"target\": [item['target']]})\n",
    "    return ref_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d157371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing dataset constructing function from utilities with a custom one.\n",
    "def parse_data(t_split='train'):\n",
    "  # Get dataset.\n",
    "  #squad_dataset = load_dataset('squad') # Method A - standard version\n",
    "  #squad_dataset, info = tfds.load('squad_question_generation/split_zhou', with_info=True, split='train') # Method B - version with split that could be better.\n",
    "\n",
    "  # Split handling - validation set further split into 50% dev/test.\n",
    "  if t_split == 'train':\n",
    "    df = pd.DataFrame(load_dataset('squad')['train'])\n",
    "  elif t_split in ['val','test']:\n",
    "    vt_df = pd.DataFrame(load_dataset('squad')['validation'])\n",
    "    df_val = vt_df.sample(frac=0.5,random_state=266)\n",
    "    if t_split == 'test':\n",
    "      df_test = vt_df.drop(df_val.index)\n",
    "      df = df_test\n",
    "    else:\n",
    "      df = df_val\n",
    "  else:\n",
    "    raise Exception(\"Invalid choice of dataset split.\")\n",
    "  \n",
    "\n",
    "  df['answer_text'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "  df['source'] = 'answer: ' + df['answer_text'] + ' context: ' + df['context'] + '</s>'\n",
    "  df['target'] = df['question']\n",
    "\n",
    "  return df                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28825349",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transformers.file_utils import is_apex_available\n",
    "\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "class Seq2SeqTrainer(Trainer):\n",
    "    \"\"\"Class to finetune a Seq2Seq model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_beams=4,\n",
    "            max_length=32,\n",
    "            *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_beams = num_beams\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        outputs = model(input_ids=inputs['input_ids'],\n",
    "                        # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                        labels=inputs['labels'])\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            return self.label_smoother(outputs, inputs[\"labels\"])\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            return outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        Runs the model to either generate a sequence and/or compute the loss.\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        # Compute loss with labels first.\n",
    "        with torch.no_grad():\n",
    "            if self.args.fp16 and _use_native_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids=inputs['input_ids'],\n",
    "                                    # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                                    labels=inputs['labels'])\n",
    "            else:\n",
    "                outputs = model(input_ids=inputs['input_ids'],\n",
    "                                # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                                labels=inputs['labels'])\n",
    "            if has_labels:\n",
    "                loss = outputs[0].mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "        # If we're only computing the conditional log-likelihood, return.\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        # Otherwise run model.generate() to get predictions.\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            preds = model.module.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        else:\n",
    "            preds = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        if len(preds) == 1:\n",
    "            preds = preds[0]\n",
    "        # Pad predictions if necessary so they can be concatenated across batches.\n",
    "        if preds.shape[-1] < self.max_length:\n",
    "            preds = torch.nn.functional.pad(\n",
    "                preds, (0, self.max_length - preds.shape[-1]),\n",
    "                mode='constant',\n",
    "                value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        # Post-process labels.\n",
    "        if has_labels:\n",
    "            labels = inputs.get('labels')\n",
    "        else:\n",
    "            labels = None\n",
    "        return (loss, preds, labels)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Load the dataset\n",
    "    #trn_df = parse_data(in_file=f'../../data/{args.dataset}/trn.tsv')\n",
    "    #val_df = parse_data(in_file=f'../../data/{args.dataset}/val.tsv')\n",
    "    trn_df = parse_data('train')\n",
    "    val_df = parse_data('val')\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    ckpt_path = None\n",
    "    if args.task == 'train':\n",
    "        ckpt_path = args.model_name\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        # update timestamp and create new path for ckpt\n",
    "        args.timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    train_data_tokenized = batchify_data(trn_df, tokenizer, args)\n",
    "    valid_data_tokenized = batchify_data(val_df, tokenizer, args)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "    model = model.to('cuda:0')\n",
    "\n",
    "    # Training Setup\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=300,\n",
    "        logging_steps=100,\n",
    "        # optimization args, the trainer uses the Adam optimizer\n",
    "        # and has a linear warmup for the learning rate\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-04,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=0,\n",
    "        lr_scheduler_type='constant',\n",
    "        # misc args\n",
    "        seed=42,\n",
    "        save_total_limit=1,  # limit the total amount of checkpoints\n",
    "        disable_tqdm=False,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,\n",
    "        local_rank=args.local_rank\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        num_beams=args.beam_size,\n",
    "        max_length=args.decoder_max_length,\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_data_tokenized,\n",
    "        eval_dataset=valid_data_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Now that we have the trainer set up, we can finetune.\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def beam_generate_sentences(batch,\n",
    "                            model,\n",
    "                            tokenizer,\n",
    "                            args,\n",
    "                            device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "    # Generate with beam search.\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        num_beams=args.beam_size,\n",
    "        max_length=args.max_generation_length,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    # Use model tokenizer to decode to text.\n",
    "    generated_sentences = [\n",
    "        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "        for gen_ids in generated_ids\n",
    "    ]\n",
    "    print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "\n",
    "def sample_sentences(batch,\n",
    "                     model,\n",
    "                     tokenizer,\n",
    "                     args,\n",
    "                     device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "\n",
    "    generated_sentences = []\n",
    "    for i in range(args.num_return_sequences):\n",
    "        # Generate with beam search.\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=features['input_ids'],\n",
    "            attention_mask=features['attention_mask'],\n",
    "            num_beams=args.beam_size,\n",
    "            max_length=args.max_generation_length,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        # Use model tokenizer to decode to text.\n",
    "        generated_sentences += [\n",
    "            tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "            for gen_ids in generated_ids\n",
    "        ]\n",
    "    print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    te_df = parse_data('test')\n",
    "    print('Data loaded!!!')\n",
    "\n",
    "    # Load the model\n",
    "    if args.timestamp == '0':\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(f\"{args.model_name}\")\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    if args.timestamp == '0':\n",
    "        model = T5ForConditionalGeneration.from_pretrained(f\"{args.model_name}\")\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        model = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "    model = model.to('cuda:0')\n",
    "    model.kernel_v = args.kernel_v\n",
    "    model.kernel_r = args.kernel_r\n",
    "    model.from_mean = args.from_mean\n",
    "    model.scaler = args.scaler\n",
    "\n",
    "    # Make predictions\n",
    "    if args.from_mean:\n",
    "        test_output = Dataset.from_pandas(te_df).map(\n",
    "            lambda batch: {'generated': beam_generate_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "    else:\n",
    "        test_output = Dataset.from_pandas(te_df).map(\n",
    "            lambda batch: {'generated': sample_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "    # prepare evaluation data\n",
    "    ref_list, pred_list = prepare_eval(list(test_output))\n",
    "    reference_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": ref_list,\n",
    "    }\n",
    "    prediction_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": pred_list,\n",
    "    }\n",
    "\n",
    "    if args.timestamp == '0':\n",
    "        os.makedirs(f\"{args.model_name}_{args.dataset}_{args.flag}_{args.timestamp}\")\n",
    "\n",
    "    with open(\n",
    "            f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/refs.json\",\n",
    "            'w') as f:\n",
    "        f.write(json.dumps(reference_dict, indent=2))\n",
    "    if args.from_mean:\n",
    "        with open(\n",
    "                f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/outs_mean.json\",\n",
    "                'w') as f:\n",
    "            f.write(json.dumps(prediction_dict, indent=2))\n",
    "    else:\n",
    "        with open(\n",
    "                f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/outs.json\",\n",
    "                'w') as f:\n",
    "            f.write(json.dumps(prediction_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239a3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee3462d6ea5496f8fa9ec365b07feec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f386ade7303a4959be5b522de694ffa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function batchify_data.<locals>.<lambda> at 0x7fe2251be8c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c274796014fb4122aabb93b717bb6409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca59d4d0b6e439ab999b6128180efc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: context, answer_text, question, source, target, title, answers, id. If context, answer_text, question, source, target, title, answers, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/ec2-user/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 87599\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 54750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='356' max='54750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  356/54750 05:05 < 13:02:28, 1.16 it/s, Epoch 0.06/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>0.539029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: __index_level_0__, context, answer_text, question, source, target, title, answers, id. If __index_level_0__, context, answer_text, question, source, target, title, answers, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5285\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to t5-base_GYAFC/em_t5base_64.0_0.0001_2022-07-12-02-06-23/checkpoint-300\n",
      "Configuration saved in t5-base_GYAFC/em_t5base_64.0_0.0001_2022-07-12-02-06-23/checkpoint-300/config.json\n",
      "Model weights saved in t5-base_GYAFC/em_t5base_64.0_0.0001_2022-07-12-02-06-23/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in t5-base_GYAFC/em_t5base_64.0_0.0001_2022-07-12-02-06-23/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in t5-base_GYAFC/em_t5base_64.0_0.0001_2022-07-12-02-06-23/checkpoint-300/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5192/4063647754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d-%H-%M-%S\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ft'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5192/1491141375.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# Now that we have the trainer set up, we can finetune.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1649\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5192/1491141375.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         outputs = model(input_ids=inputs['input_ids'],\n\u001b[1;32m     30\u001b[0m                         \u001b[0;31m# decoder_input_ids=inputs['labels'][:,:-1],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         labels=inputs['labels'])\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m         )\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 )\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         )\n\u001b[1;32m    579\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = argparse.ArgumentParser(description='Hyperparams')\n",
    "p.add_argument('-t', '--task', type=str, default=\"train\",\n",
    "                help=\"specify the task to do: (train)ing, ft(finetune), (eval)uation\")\n",
    "#p.add_argument('-t', '--task', type=str, default=\"ft\",\n",
    "#                help=\"specify the task to do: (train)ing, ft(finetune), (eval)uation\")\n",
    "p.add_argument('-c', '--ckpt', type=str, default=\"193280\",\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-time', '--timestamp', type=str, default='2021-02-14-04-57-04',\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-f', '--flag', type=str, default='gpvae',\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-d', '--dataset', type=str, default=\"GYAFC/em\",\n",
    "                help=\"specify the dataset: GYAFC/em, GYAFC/fr\")\n",
    "p.add_argument('--model_name', type=str, default=\"t5-base\",\n",
    "                help=\"specify the model name: t5-base, facebook/blenderbot-400M-distill\")\n",
    "p.add_argument('-v', '--kernel_v', type=float, default=64.0,\n",
    "                help=\"Hyper-parameter for prior kernel,  control the signal variance\")\n",
    "p.add_argument('-r', '--kernel_r', type=float, default=0.0001,\n",
    "                help=\"Hyper-parameter for prior kernel.\")\n",
    "p.add_argument('-s', '--scaler', type=float, default=1.0)\n",
    "p.add_argument('--from_mean', action='store_true',\n",
    "                help=\"specify whether sample from mean during generation\")\n",
    "#p.add_argument('-bz', '--batch_size', type=int, default=16)\n",
    "p.add_argument('-bz', '--batch_size', type=int, default=16)\n",
    "p.add_argument('-e', '--epochs', type=int, default=10)\n",
    "#p.add_argument('--encoder_max_length', type=int, default=50)\n",
    "p.add_argument('--encoder_max_length', type=int, default=128)\n",
    "p.add_argument('--decoder_max_length', type=int, default=48)\n",
    "#p.add_argument('--max_generation_length', type=int, default=60)\n",
    "p.add_argument('--max_generation_length', type=int, default=96)\n",
    "#p.add_argument('--beam_size', type=int, default=10)\n",
    "p.add_argument('--beam_size', type=int, default=5)\n",
    "#p.add_argument('--num_return_sequences', type=int, default=10)\n",
    "p.add_argument('--num_return_sequences', type=int, default=5)\n",
    "p.add_argument('--local_rank', type=int, default=-1,\n",
    "                help=\"Multiple GPU training\")\n",
    "args = p.parse_args()\n",
    "\n",
    "# jupyter fix for bad flag\n",
    "args.flag = 't5base'\n",
    "\n",
    "if args.task == 'train':\n",
    "    args.timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "    train(args)\n",
    "elif args.task == 'ft':\n",
    "    train(args)\n",
    "else:\n",
    "    test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
