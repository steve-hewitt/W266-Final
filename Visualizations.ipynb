{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e9d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83efe993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather data\n",
    "df_base_beam = pd.read_pickle('df_baseline256.pkl')\n",
    "df_base_nucleus = pd.read_pickle('df_baseline256_NS.pkl')\n",
    "df_GPP_beam = pd.read_pickle('df_GPP256.pkl')\n",
    "df_GPP_nucleus = pd.read_pickle('df_GPP256_NS.pkl')\n",
    "\n",
    "# Combine\n",
    "d = {'Reference' : df_base_beam['Reference'].to_list(),\n",
    "'Answer': df_base_beam['Answer'].to_list(),\n",
    "'Context': df_base_beam['Context'].to_list(),\n",
    "'Base.BS.Prediction': df_base_beam['Prediction'].to_list(),\n",
    "'Base.BS.BLEU': df_base_beam['BLEU'].to_list(),\n",
    "'Base.BS.SacreBLEU': df_base_beam['SacreBLEU'].to_list(),\n",
    "'Base.BS.METEOR': df_base_beam['METEOR'].to_list(),\n",
    "'Base.BS.ROUGE': df_base_beam['ROUGE'].to_list(),\n",
    "'Base.BS.AC': df_base_beam['Answer_Contamination'].to_list(),\n",
    "'Base.NS.Prediction': df_base_nucleus['Prediction'].to_list(),\n",
    "'Base.NS.BLEU': df_base_nucleus['BLEU'].to_list(),\n",
    "'Base.NS.SacreBLEU': df_base_nucleus['SacreBLEU'].to_list(),\n",
    "'Base.NS.METEOR': df_base_nucleus['METEOR'].to_list(),\n",
    "'Base.NS.ROUGE': df_base_nucleus['ROUGE'].to_list(),\n",
    "'Base.NS.AC': df_base_nucleus['Answer_Contamination'].to_list(), \n",
    "'GPP.BS.Prediction': df_GPP_beam['Prediction'].to_list(),\n",
    "'GPP.BS.BLEU': df_GPP_beam['BLEU'].to_list(),\n",
    "'GPP.BS.SacreBLEU': df_GPP_beam['SacreBLEU'].to_list(),\n",
    "'GPP.BS.METEOR': df_GPP_beam['METEOR'].to_list(),\n",
    "'GPP.BS.ROUGE': df_GPP_beam['ROUGE'].to_list(),\n",
    "'GPP.BS.AC': df_GPP_beam['Answer_Contamination'].to_list(),\n",
    "'GPP.NS.Prediction': df_GPP_nucleus['Prediction'].to_list(),\n",
    "'GPP.NS.BLEU': df_GPP_nucleus['BLEU'].to_list(),\n",
    "'GPP.NS.SacreBLEU': df_GPP_nucleus['SacreBLEU'].to_list(),\n",
    "'GPP.NS.METEOR': df_GPP_nucleus['METEOR'].to_list(),\n",
    "'GPP.NS.ROUGE': df_GPP_nucleus['ROUGE'].to_list(),\n",
    "'GPP.NS.AC': df_GPP_nucleus['Answer_Contamination'].to_list()}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "# Calculate SacreBLEU differentials between models.\n",
    "df['Base.v.GPP.Beam.SacreBLEU'] = df['Base.BS.SacreBLEU'] - df['GPP.BS.SacreBLEU']\n",
    "df['Base.v.GPP.NS.SacreBLEU'] = df['Base.NS.SacreBLEU'] - df['GPP.NS.SacreBLEU']\n",
    "\n",
    "# Calculate SacreBLEU differentials between decoding strategies.\n",
    "df['Base.Beam.vs.NS.SacreBLEU'] = df['Base.BS.SacreBLEU'] - df['Base.NS.SacreBLEU']\n",
    "df['GPP.Beam.vs.NS.SacreBLEU'] = df['GPP.BS.SacreBLEU'] - df['GPP.NS.SacreBLEU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98813126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>SacreBLEU</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>Perfect Predictions</th>\n",
       "      <th>Best Predictions</th>\n",
       "      <th>Worst Predictions</th>\n",
       "      <th>Answer Contamination</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline (BS)</th>\n",
       "      <td>0.211274</td>\n",
       "      <td>21.127372</td>\n",
       "      <td>0.462751</td>\n",
       "      <td>0.474482</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>0.195270</td>\n",
       "      <td>0.112772</td>\n",
       "      <td>0.007569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline (NS)</th>\n",
       "      <td>0.182588</td>\n",
       "      <td>18.258757</td>\n",
       "      <td>0.442596</td>\n",
       "      <td>0.439014</td>\n",
       "      <td>0.026112</td>\n",
       "      <td>0.164995</td>\n",
       "      <td>0.144560</td>\n",
       "      <td>0.007947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPP (BS)</th>\n",
       "      <td>0.165659</td>\n",
       "      <td>16.565876</td>\n",
       "      <td>0.401850</td>\n",
       "      <td>0.414320</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.156102</td>\n",
       "      <td>0.222517</td>\n",
       "      <td>0.132640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPP (NS)</th>\n",
       "      <td>0.136425</td>\n",
       "      <td>13.642460</td>\n",
       "      <td>0.373236</td>\n",
       "      <td>0.374232</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>0.148723</td>\n",
       "      <td>0.283254</td>\n",
       "      <td>0.083822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   BLEU  SacreBLEU   ROUGE-L    METEOR  Perfect Predictions  \\\n",
       "Model                                                                         \n",
       "Baseline (BS)  0.211274  21.127372  0.462751  0.474482             0.032167   \n",
       "Baseline (NS)  0.182588  18.258757  0.442596  0.439014             0.026112   \n",
       "GPP (BS)       0.165659  16.565876  0.401850  0.414320             0.020057   \n",
       "GPP (NS)       0.136425  13.642460  0.373236  0.374232             0.011921   \n",
       "\n",
       "               Best Predictions  Worst Predictions  Answer Contamination  \n",
       "Model                                                                     \n",
       "Baseline (BS)          0.195270           0.112772              0.007569  \n",
       "Baseline (NS)          0.164995           0.144560              0.007947  \n",
       "GPP (BS)               0.156102           0.222517              0.132640  \n",
       "GPP (NS)               0.148723           0.283254              0.083822  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get high level set stats.\n",
    "base_BS_perfect = len(df[df['Base.BS.SacreBLEU'] > 99.9])\n",
    "base_NS_perfect = len(df[df['Base.NS.SacreBLEU'] > 99.9])\n",
    "GPP_BS_perfect = len(df[df['GPP.BS.SacreBLEU'] > 99.9])\n",
    "GPP_NS_perfect = len(df[df['GPP.NS.SacreBLEU'] > 99.9])\n",
    "base_BS_best = len(df[(df['Base.BS.SacreBLEU'] > df['Base.NS.SacreBLEU']) & (df['Base.BS.SacreBLEU'] > df['GPP.BS.SacreBLEU']) & (df['Base.BS.SacreBLEU'] > df['GPP.NS.SacreBLEU'])])\n",
    "base_NS_best = len(df[(df['Base.NS.SacreBLEU'] > df['Base.BS.SacreBLEU']) & (df['Base.NS.SacreBLEU'] > df['GPP.BS.SacreBLEU']) & (df['Base.NS.SacreBLEU'] > df['GPP.NS.SacreBLEU'])])\n",
    "GPP_BS_best = len(df[(df['GPP.BS.SacreBLEU'] > df['Base.NS.SacreBLEU']) & (df['GPP.BS.SacreBLEU'] > df['Base.BS.SacreBLEU']) & (df['GPP.BS.SacreBLEU'] > df['GPP.NS.SacreBLEU'])])\n",
    "GPP_NS_best = len(df[(df['GPP.NS.SacreBLEU'] > df['Base.NS.SacreBLEU']) & (df['GPP.NS.SacreBLEU'] > df['GPP.BS.SacreBLEU']) & (df['GPP.NS.SacreBLEU'] > df['Base.BS.SacreBLEU'])])\n",
    "base_BS_worst = len(df[(df['Base.BS.SacreBLEU'] < df['Base.NS.SacreBLEU']) & (df['Base.BS.SacreBLEU'] < df['GPP.BS.SacreBLEU']) & (df['Base.BS.SacreBLEU'] < df['GPP.NS.SacreBLEU'])])\n",
    "base_NS_worst = len(df[(df['Base.NS.SacreBLEU'] < df['Base.BS.SacreBLEU']) & (df['Base.NS.SacreBLEU'] < df['GPP.BS.SacreBLEU']) & (df['Base.NS.SacreBLEU'] < df['GPP.NS.SacreBLEU'])])\n",
    "GPP_BS_worst = len(df[(df['GPP.BS.SacreBLEU'] < df['Base.NS.SacreBLEU']) & (df['GPP.BS.SacreBLEU'] < df['Base.BS.SacreBLEU']) & (df['GPP.BS.SacreBLEU'] < df['GPP.NS.SacreBLEU'])])\n",
    "GPP_NS_worst = len(df[(df['GPP.NS.SacreBLEU'] < df['Base.NS.SacreBLEU']) & (df['GPP.NS.SacreBLEU'] < df['GPP.BS.SacreBLEU']) & (df['GPP.NS.SacreBLEU'] < df['Base.BS.SacreBLEU'])])\n",
    "base_BS_AC = len(df[df['Base.BS.AC'] == True])\n",
    "base_NS_AC = len(df[df['Base.NS.AC'] == True])\n",
    "GPP_BS_AC = len(df[df['GPP.BS.AC'] == True])\n",
    "GPP_NS_AC = len(df[df['GPP.NS.AC'] == True])\n",
    "\n",
    "# Create DataFrame of results.\n",
    "c = {'Model': ['Baseline (BS)', 'Baseline (NS)', 'GPP (BS)', 'GPP (NS)'],\n",
    "     'BLEU': [0.21127372392801158, 0.18258756628554812, 0.1656587608025032, 0.1364246031118634],\n",
    "     'SacreBLEU' : [21.127372392801167, 18.25875662855481, 16.56587608025032, 13.64246031118634],\n",
    "     'ROUGE-L' : [0.4627507125088821, 0.44259606326080936, 0.4018497255426625, 0.3732356714376023],\n",
    "     'METEOR': [0.47448171105801523, 0.4390140638504702, 0.4143196855920803, 0.37423245714266146],\n",
    "     'Perfect Predictions' : [base_BS_perfect, base_NS_perfect, GPP_BS_perfect, GPP_NS_perfect],\n",
    "     'Best Predictions' : [base_BS_best, base_NS_best, GPP_BS_best, GPP_NS_best],\n",
    "     'Worst Predictions': [base_BS_worst, base_NS_worst, GPP_BS_worst, GPP_NS_worst],\n",
    "     'Answer Contamination': [base_BS_AC, base_NS_AC, GPP_BS_AC, GPP_NS_AC]}\n",
    "cf = pd.DataFrame(c)\n",
    "\n",
    "# Convert values to percentages.\n",
    "cf['Perfect Predictions'] = cf['Perfect Predictions'].div(len(df))\n",
    "cf['Best Predictions'] = cf['Best Predictions'].div(len(df))\n",
    "cf['Worst Predictions'] = cf['Worst Predictions'].div(len(df))\n",
    "cf['Answer Contamination'] = cf['Answer Contamination'].div(len(df))\n",
    "\n",
    "# Dispaly results\n",
    "cf.set_index('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e0bca0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Base.BS.Prediction</th>\n",
       "      <th>GPP.BS.Prediction</th>\n",
       "      <th>Base.BS.SacreBLEU</th>\n",
       "      <th>GPP.BS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>Who has loaned the Raphael Cartoons to the mus...</td>\n",
       "      <td>Queen Elizabeth II</td>\n",
       "      <td>Who loaned the Raphael Cartoons to the museum?</td>\n",
       "      <td>Who was the Queen Elizabeth II?</td>\n",
       "      <td>79.563717</td>\n",
       "      <td>5.630401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>What collection does the V&amp;A Theatre &amp; Perform...</td>\n",
       "      <td>material about live performance</td>\n",
       "      <td>What do the V&amp;A Theatre &amp; Performance gallerie...</td>\n",
       "      <td>What is the UK's largest national collection o...</td>\n",
       "      <td>76.043218</td>\n",
       "      <td>4.368584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>How many hymns did Luther write based on the T...</td>\n",
       "      <td>two hymns</td>\n",
       "      <td>How many hymns did Luther write on the Ten Com...</td>\n",
       "      <td>What was Luther's main hymn for Christmas?</td>\n",
       "      <td>74.261411</td>\n",
       "      <td>3.349504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>What museum specializes in cultural history an...</td>\n",
       "      <td>Peabody Museum of Archaeology and Ethnology</td>\n",
       "      <td>What museum specializes in the cultural histor...</td>\n",
       "      <td>What is the name of the Peabody Museum of Arch...</td>\n",
       "      <td>78.254229</td>\n",
       "      <td>7.431878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>What did Luther do at the end of his speech?</td>\n",
       "      <td>raised his arm</td>\n",
       "      <td>What did Luther do at the end of the speech?</td>\n",
       "      <td>Luther raised his arm in the traditional salut...</td>\n",
       "      <td>74.194466</td>\n",
       "      <td>3.716499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "2773  Who has loaned the Raphael Cartoons to the mus...   \n",
       "2803  What collection does the V&A Theatre & Perform...   \n",
       "1211  How many hymns did Luther write based on the T...   \n",
       "3569  What museum specializes in cultural history an...   \n",
       "1129       What did Luther do at the end of his speech?   \n",
       "\n",
       "                                           Answer  \\\n",
       "2773                           Queen Elizabeth II   \n",
       "2803              material about live performance   \n",
       "1211                                    two hymns   \n",
       "3569  Peabody Museum of Archaeology and Ethnology   \n",
       "1129                               raised his arm   \n",
       "\n",
       "                                     Base.BS.Prediction  \\\n",
       "2773     Who loaned the Raphael Cartoons to the museum?   \n",
       "2803  What do the V&A Theatre & Performance gallerie...   \n",
       "1211  How many hymns did Luther write on the Ten Com...   \n",
       "3569  What museum specializes in the cultural histor...   \n",
       "1129       What did Luther do at the end of the speech?   \n",
       "\n",
       "                                      GPP.BS.Prediction  Base.BS.SacreBLEU  \\\n",
       "2773                    Who was the Queen Elizabeth II?          79.563717   \n",
       "2803  What is the UK's largest national collection o...          76.043218   \n",
       "1211         What was Luther's main hymn for Christmas?          74.261411   \n",
       "3569  What is the name of the Peabody Museum of Arch...          78.254229   \n",
       "1129  Luther raised his arm in the traditional salut...          74.194466   \n",
       "\n",
       "      GPP.BS.SacreBLEU  \n",
       "2773          5.630401  \n",
       "2803          4.368584  \n",
       "1211          3.349504  \n",
       "3569          7.431878  \n",
       "1129          3.716499  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where Base (Beam) performs much better than GPP (Beam), but isn't perfect.\n",
    "df[df['Base.BS.SacreBLEU'] < 80].sort_values(by = 'Base.v.GPP.Beam.SacreBLEU', ascending = False)[['Reference','Answer','Base.BS.Prediction','GPP.BS.Prediction','Base.BS.SacreBLEU','GPP.BS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b422723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Base.BS.Prediction</th>\n",
       "      <th>GPP.BS.Prediction</th>\n",
       "      <th>Base.BS.SacreBLEU</th>\n",
       "      <th>GPP.BS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>What compiles and reports on data about the si...</td>\n",
       "      <td>ENR</td>\n",
       "      <td>What is a trade magazine for the construction ...</td>\n",
       "      <td>What magazine compiles and reports data about ...</td>\n",
       "      <td>3.599276</td>\n",
       "      <td>73.769925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Who does the statue of Little Insurgent commem...</td>\n",
       "      <td>children</td>\n",
       "      <td>Who served as messengers and frontline troops ...</td>\n",
       "      <td>What did the statue of Little Insurgent commem...</td>\n",
       "      <td>4.456883</td>\n",
       "      <td>72.597953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>How high is the highest point in Warsaw?</td>\n",
       "      <td>452.8 ft</td>\n",
       "      <td>How tall is Szczliwice?</td>\n",
       "      <td>What is the highest point in Warsaw?</td>\n",
       "      <td>6.316906</td>\n",
       "      <td>74.208848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>What year did Newcastle first develop its cycl...</td>\n",
       "      <td>1998</td>\n",
       "      <td>When was Newcastle's cycling strategy first de...</td>\n",
       "      <td>When did Newcastle first develop its cycling s...</td>\n",
       "      <td>10.229197</td>\n",
       "      <td>77.255059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>When did President Uhuru Kenyatta sign a Secur...</td>\n",
       "      <td>December 2014</td>\n",
       "      <td>When was the Security Laws Amendment Bill signed?</td>\n",
       "      <td>When did President Uhuru Kenyatta sign a Secur...</td>\n",
       "      <td>8.591317</td>\n",
       "      <td>73.488892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference         Answer  \\\n",
       "3432  What compiles and reports on data about the si...            ENR   \n",
       "507   Who does the statue of Little Insurgent commem...       children   \n",
       "490            How high is the highest point in Warsaw?       452.8 ft   \n",
       "2634  What year did Newcastle first develop its cycl...           1998   \n",
       "4201  When did President Uhuru Kenyatta sign a Secur...  December 2014   \n",
       "\n",
       "                                     Base.BS.Prediction  \\\n",
       "3432  What is a trade magazine for the construction ...   \n",
       "507   Who served as messengers and frontline troops ...   \n",
       "490                             How tall is Szczliwice?   \n",
       "2634  When was Newcastle's cycling strategy first de...   \n",
       "4201  When was the Security Laws Amendment Bill signed?   \n",
       "\n",
       "                                      GPP.BS.Prediction  Base.BS.SacreBLEU  \\\n",
       "3432  What magazine compiles and reports data about ...           3.599276   \n",
       "507   What did the statue of Little Insurgent commem...           4.456883   \n",
       "490                What is the highest point in Warsaw?           6.316906   \n",
       "2634  When did Newcastle first develop its cycling s...          10.229197   \n",
       "4201  When did President Uhuru Kenyatta sign a Secur...           8.591317   \n",
       "\n",
       "      GPP.BS.SacreBLEU  \n",
       "3432         73.769925  \n",
       "507          72.597953  \n",
       "490          74.208848  \n",
       "2634         77.255059  \n",
       "4201         73.488892  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where GPP (Beam) performs much better than Base (Beam), but isn't perfect.\n",
    "df[df['GPP.BS.SacreBLEU'] < 80].sort_values(by = 'Base.v.GPP.Beam.SacreBLEU', ascending = True)[['Reference','Answer','Base.BS.Prediction','GPP.BS.Prediction','Base.BS.SacreBLEU','GPP.BS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8c1ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Base.NS.Prediction</th>\n",
       "      <th>GPP.NS.Prediction</th>\n",
       "      <th>Base.NS.SacreBLEU</th>\n",
       "      <th>GPP.NS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>Which courts have a duty to interpret domestic...</td>\n",
       "      <td>national courts</td>\n",
       "      <td>Who has a duty to interpret domestic law as fa...</td>\n",
       "      <td>What does Francovich v Italy say they can do?</td>\n",
       "      <td>76.321115</td>\n",
       "      <td>2.812740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>Which British monarch appears above the frame ...</td>\n",
       "      <td>Queen Victoria</td>\n",
       "      <td>Who appears above the frame around the arches ...</td>\n",
       "      <td>What is the Queen Victoria's title?</td>\n",
       "      <td>74.466974</td>\n",
       "      <td>3.314288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>What museum specializes in cultural history an...</td>\n",
       "      <td>Peabody Museum of Archaeology and Ethnology</td>\n",
       "      <td>What museum specializes in the cultural histor...</td>\n",
       "      <td>What museum does the Harvard Museum of Natural...</td>\n",
       "      <td>78.254229</td>\n",
       "      <td>7.858254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4530</th>\n",
       "      <td>Which conjecture holds that every even integer...</td>\n",
       "      <td>Goldbach's conjecture</td>\n",
       "      <td>What claims that every even integer n greater ...</td>\n",
       "      <td>What does Goldbach's conjecture assert?</td>\n",
       "      <td>71.183293</td>\n",
       "      <td>0.792320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>How many hymns did Luther write based on the T...</td>\n",
       "      <td>two hymns</td>\n",
       "      <td>How many hymns did Luther write on the Ten Com...</td>\n",
       "      <td>What was the name of Luther's main hymn?</td>\n",
       "      <td>74.261411</td>\n",
       "      <td>4.062583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "2080  Which courts have a duty to interpret domestic...   \n",
       "2687  Which British monarch appears above the frame ...   \n",
       "3569  What museum specializes in cultural history an...   \n",
       "4530  Which conjecture holds that every even integer...   \n",
       "1211  How many hymns did Luther write based on the T...   \n",
       "\n",
       "                                           Answer  \\\n",
       "2080                              national courts   \n",
       "2687                               Queen Victoria   \n",
       "3569  Peabody Museum of Archaeology and Ethnology   \n",
       "4530                        Goldbach's conjecture   \n",
       "1211                                    two hymns   \n",
       "\n",
       "                                     Base.NS.Prediction  \\\n",
       "2080  Who has a duty to interpret domestic law as fa...   \n",
       "2687  Who appears above the frame around the arches ...   \n",
       "3569  What museum specializes in the cultural histor...   \n",
       "4530  What claims that every even integer n greater ...   \n",
       "1211  How many hymns did Luther write on the Ten Com...   \n",
       "\n",
       "                                      GPP.NS.Prediction  Base.NS.SacreBLEU  \\\n",
       "2080      What does Francovich v Italy say they can do?          76.321115   \n",
       "2687                What is the Queen Victoria's title?          74.466974   \n",
       "3569  What museum does the Harvard Museum of Natural...          78.254229   \n",
       "4530            What does Goldbach's conjecture assert?          71.183293   \n",
       "1211           What was the name of Luther's main hymn?          74.261411   \n",
       "\n",
       "      GPP.NS.SacreBLEU  \n",
       "2080          2.812740  \n",
       "2687          3.314288  \n",
       "3569          7.858254  \n",
       "4530          0.792320  \n",
       "1211          4.062583  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where Base (Nucleus) performs much better than GPP (Nucleus), but isn't perfect.\n",
    "df[df['Base.NS.SacreBLEU'] < 80].sort_values(by = 'Base.v.GPP.NS.SacreBLEU', ascending = False)[['Reference','Answer','Base.NS.Prediction','GPP.NS.Prediction','Base.NS.SacreBLEU','GPP.NS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f0aca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Base.NS.Prediction</th>\n",
       "      <th>GPP.NS.Prediction</th>\n",
       "      <th>Base.NS.SacreBLEU</th>\n",
       "      <th>GPP.NS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>In what year did ENR compile data in nine mark...</td>\n",
       "      <td>2014</td>\n",
       "      <td>When was ENR compiled?</td>\n",
       "      <td>In what year did ENR compile data on nine mark...</td>\n",
       "      <td>3.132600</td>\n",
       "      <td>73.488892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>What compiles and reports on data about the si...</td>\n",
       "      <td>ENR</td>\n",
       "      <td>What is the trade magazine for the constructio...</td>\n",
       "      <td>What magazine compiles and reports data about ...</td>\n",
       "      <td>3.599276</td>\n",
       "      <td>73.769925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>What is the name of the stadium in Miami that ...</td>\n",
       "      <td>Sun Life Stadium</td>\n",
       "      <td>What stadium was Miami's?</td>\n",
       "      <td>What is the name of the stadium in Miami?</td>\n",
       "      <td>3.050026</td>\n",
       "      <td>66.940483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>When did ABC's New York flagship stations chan...</td>\n",
       "      <td>May 1, 1953</td>\n",
       "      <td>When did ABC change their callsigns to WABC, W...</td>\n",
       "      <td>When did ABC's New York flagship stations chan...</td>\n",
       "      <td>9.238430</td>\n",
       "      <td>72.925717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4639</th>\n",
       "      <td>What city has the largest inland port in Europe?</td>\n",
       "      <td>Duisburg</td>\n",
       "      <td>Where is the Rhine a hub?</td>\n",
       "      <td>What is the largest inland port in Europe?</td>\n",
       "      <td>5.087641</td>\n",
       "      <td>67.168774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference            Answer  \\\n",
       "3433  In what year did ENR compile data in nine mark...              2014   \n",
       "3432  What compiles and reports on data about the si...               ENR   \n",
       "62    What is the name of the stadium in Miami that ...  Sun Life Stadium   \n",
       "2880  When did ABC's New York flagship stations chan...       May 1, 1953   \n",
       "4639   What city has the largest inland port in Europe?          Duisburg   \n",
       "\n",
       "                                     Base.NS.Prediction  \\\n",
       "3433                             When was ENR compiled?   \n",
       "3432  What is the trade magazine for the constructio...   \n",
       "62                            What stadium was Miami's?   \n",
       "2880  When did ABC change their callsigns to WABC, W...   \n",
       "4639                          Where is the Rhine a hub?   \n",
       "\n",
       "                                      GPP.NS.Prediction  Base.NS.SacreBLEU  \\\n",
       "3433  In what year did ENR compile data on nine mark...           3.132600   \n",
       "3432  What magazine compiles and reports data about ...           3.599276   \n",
       "62            What is the name of the stadium in Miami?           3.050026   \n",
       "2880  When did ABC's New York flagship stations chan...           9.238430   \n",
       "4639         What is the largest inland port in Europe?           5.087641   \n",
       "\n",
       "      GPP.NS.SacreBLEU  \n",
       "3433         73.488892  \n",
       "3432         73.769925  \n",
       "62           66.940483  \n",
       "2880         72.925717  \n",
       "4639         67.168774  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where GPP (Nucleus) performs much better than Base (Nucleus), but isn't perfect.\n",
    "df[df['GPP.NS.SacreBLEU'] < 80].sort_values(by = 'Base.v.GPP.NS.SacreBLEU', ascending = True)[['Reference','Answer','Base.NS.Prediction','GPP.NS.Prediction','Base.NS.SacreBLEU','GPP.NS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f5855e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Base.BS.Prediction</th>\n",
       "      <th>Base.NS.Prediction</th>\n",
       "      <th>Base.BS.SacreBLEU</th>\n",
       "      <th>Base.NS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>What did Olivier Roy state underwent a remarka...</td>\n",
       "      <td>Sunni pan-Islamism</td>\n",
       "      <td>What religion underwent a remarkable shift in ...</td>\n",
       "      <td>What movement was eclipsed by the Salafi movem...</td>\n",
       "      <td>70.954611</td>\n",
       "      <td>2.308317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>What was the capital of the Ottoman empire?</td>\n",
       "      <td>Istanbul</td>\n",
       "      <td>What was the capital of the Ottoman Empire?</td>\n",
       "      <td>What city was Ottoman Empire's capital?</td>\n",
       "      <td>75.062385</td>\n",
       "      <td>7.379782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>What happened to the East India Trading Compan...</td>\n",
       "      <td>exploitation</td>\n",
       "      <td>What happened to the East India Company in 1767?</td>\n",
       "      <td>What was the result of political activity?</td>\n",
       "      <td>71.086679</td>\n",
       "      <td>4.995139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>What is grown in the fertile highlands?</td>\n",
       "      <td>Tea, coffee, sisal, pyrethrum, corn, and wheat</td>\n",
       "      <td>What are grown in the fertile highlands?</td>\n",
       "      <td>What are the most successful agricultural prod...</td>\n",
       "      <td>70.710678</td>\n",
       "      <td>5.300157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>What was Luther's Disputation of Martin Luther...</td>\n",
       "      <td>The Ninety-Five Theses</td>\n",
       "      <td>What was Luther's \"Disputation of Martin Luthe...</td>\n",
       "      <td>What was the name of the document that Luther ...</td>\n",
       "      <td>70.858764</td>\n",
       "      <td>6.291586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "4821  What did Olivier Roy state underwent a remarka...   \n",
       "5000        What was the capital of the Ottoman empire?   \n",
       "4960  What happened to the East India Trading Compan...   \n",
       "4221            What is grown in the fertile highlands?   \n",
       "1096  What was Luther's Disputation of Martin Luther...   \n",
       "\n",
       "                                              Answer  \\\n",
       "4821                              Sunni pan-Islamism   \n",
       "5000                                        Istanbul   \n",
       "4960                                    exploitation   \n",
       "4221  Tea, coffee, sisal, pyrethrum, corn, and wheat   \n",
       "1096                          The Ninety-Five Theses   \n",
       "\n",
       "                                     Base.BS.Prediction  \\\n",
       "4821  What religion underwent a remarkable shift in ...   \n",
       "5000        What was the capital of the Ottoman Empire?   \n",
       "4960   What happened to the East India Company in 1767?   \n",
       "4221           What are grown in the fertile highlands?   \n",
       "1096  What was Luther's \"Disputation of Martin Luthe...   \n",
       "\n",
       "                                     Base.NS.Prediction  Base.BS.SacreBLEU  \\\n",
       "4821  What movement was eclipsed by the Salafi movem...          70.954611   \n",
       "5000            What city was Ottoman Empire's capital?          75.062385   \n",
       "4960         What was the result of political activity?          71.086679   \n",
       "4221  What are the most successful agricultural prod...          70.710678   \n",
       "1096  What was the name of the document that Luther ...          70.858764   \n",
       "\n",
       "      Base.NS.SacreBLEU  \n",
       "4821           2.308317  \n",
       "5000           7.379782  \n",
       "4960           4.995139  \n",
       "4221           5.300157  \n",
       "1096           6.291586  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where Base (Beam) performs much better than Base (Nucleus), but isn't perfect.\n",
    "df[df['Base.BS.SacreBLEU'] < 80].sort_values(by = 'Base.Beam.vs.NS.SacreBLEU', ascending = False)[['Reference','Answer','Base.BS.Prediction','Base.NS.Prediction','Base.BS.SacreBLEU','Base.NS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac5b72f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Base.BS.Prediction</th>\n",
       "      <th>Base.NS.Prediction</th>\n",
       "      <th>Base.BS.SacreBLEU</th>\n",
       "      <th>Base.NS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>Which material is the Gloucester Candlestick m...</td>\n",
       "      <td>gilt bronze</td>\n",
       "      <td>What is one of the rarest items in the collect...</td>\n",
       "      <td>What is the Gloucester Candlestick made from?</td>\n",
       "      <td>4.932352</td>\n",
       "      <td>74.208848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Who was the AFC champion?</td>\n",
       "      <td>Which team won Super Bowl 50?</td>\n",
       "      <td>5.815868</td>\n",
       "      <td>72.895452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>What treaty ended the Wars of Religion?</td>\n",
       "      <td>the Edict of Nantes</td>\n",
       "      <td>What gave the Huguenots substantial religious,...</td>\n",
       "      <td>What law ended the Wars of Religion?</td>\n",
       "      <td>4.456883</td>\n",
       "      <td>70.710678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>What book did Luther read in 1542?</td>\n",
       "      <td>Qur'an</td>\n",
       "      <td>What was the Latin translation of?</td>\n",
       "      <td>What did Luther read in 1542?</td>\n",
       "      <td>6.770186</td>\n",
       "      <td>72.895452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What team was the NFC champion?</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>Who did the Denver Broncos defeat in Super Bow...</td>\n",
       "      <td>Who was the NFC champion?</td>\n",
       "      <td>4.456883</td>\n",
       "      <td>64.318702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference               Answer  \\\n",
       "2770  Which material is the Gloucester Candlestick m...          gilt bronze   \n",
       "1                     Which NFL team won Super Bowl 50?       Denver Broncos   \n",
       "1505            What treaty ended the Wars of Religion?  the Edict of Nantes   \n",
       "1243                 What book did Luther read in 1542?               Qur'an   \n",
       "12                      What team was the NFC champion?    Carolina Panthers   \n",
       "\n",
       "                                     Base.BS.Prediction  \\\n",
       "2770  What is one of the rarest items in the collect...   \n",
       "1                             Who was the AFC champion?   \n",
       "1505  What gave the Huguenots substantial religious,...   \n",
       "1243                 What was the Latin translation of?   \n",
       "12    Who did the Denver Broncos defeat in Super Bow...   \n",
       "\n",
       "                                 Base.NS.Prediction  Base.BS.SacreBLEU  \\\n",
       "2770  What is the Gloucester Candlestick made from?           4.932352   \n",
       "1                     Which team won Super Bowl 50?           5.815868   \n",
       "1505           What law ended the Wars of Religion?           4.456883   \n",
       "1243                  What did Luther read in 1542?           6.770186   \n",
       "12                        Who was the NFC champion?           4.456883   \n",
       "\n",
       "      Base.NS.SacreBLEU  \n",
       "2770          74.208848  \n",
       "1             72.895452  \n",
       "1505          70.710678  \n",
       "1243          72.895452  \n",
       "12            64.318702  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where Base (Nucleus) performs much better than Base (Beam), but isn't perfect.\n",
    "df[df['Base.NS.SacreBLEU'] < 80].sort_values(by = 'Base.Beam.vs.NS.SacreBLEU', ascending = True)[['Reference','Answer','Base.BS.Prediction','Base.NS.Prediction','Base.BS.SacreBLEU','Base.NS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "136c7730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>GPP.BS.Prediction</th>\n",
       "      <th>GPP.NS.Prediction</th>\n",
       "      <th>GPP.BS.SacreBLEU</th>\n",
       "      <th>GPP.NS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4530</th>\n",
       "      <td>Which conjecture holds that every even integer...</td>\n",
       "      <td>Goldbach's conjecture</td>\n",
       "      <td>What conjecture asserts that every even intege...</td>\n",
       "      <td>What does Goldbach's conjecture assert?</td>\n",
       "      <td>72.003913</td>\n",
       "      <td>0.792320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Who does the statue of Little Insurgent commem...</td>\n",
       "      <td>children</td>\n",
       "      <td>What did the statue of Little Insurgent commem...</td>\n",
       "      <td>What was the name of the child in the memorial?</td>\n",
       "      <td>72.597953</td>\n",
       "      <td>4.932352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>Even in large firms, architects, interior desi...</td>\n",
       "      <td>entirely separate companies</td>\n",
       "      <td>In the past, architects, interior designers, e...</td>\n",
       "      <td>What was the past trend in integrating the dif...</td>\n",
       "      <td>65.759095</td>\n",
       "      <td>1.040711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>How many total touchdowns did Cam Newton score?</td>\n",
       "      <td>45</td>\n",
       "      <td>How many total touchdowns did Cam Newton have?</td>\n",
       "      <td>What was the career-high for total touchdowns?</td>\n",
       "      <td>75.062385</td>\n",
       "      <td>10.786826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>How many fumbles did Von Miller force?</td>\n",
       "      <td>two</td>\n",
       "      <td>How many fumbles did Von Miller have?</td>\n",
       "      <td>What was Von Miller's first sack?</td>\n",
       "      <td>70.710678</td>\n",
       "      <td>6.770186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "4530  Which conjecture holds that every even integer...   \n",
       "507   Who does the statue of Little Insurgent commem...   \n",
       "3444  Even in large firms, architects, interior desi...   \n",
       "116     How many total touchdowns did Cam Newton score?   \n",
       "36               How many fumbles did Von Miller force?   \n",
       "\n",
       "                           Answer  \\\n",
       "4530        Goldbach's conjecture   \n",
       "507                      children   \n",
       "3444  entirely separate companies   \n",
       "116                            45   \n",
       "36                            two   \n",
       "\n",
       "                                      GPP.BS.Prediction  \\\n",
       "4530  What conjecture asserts that every even intege...   \n",
       "507   What did the statue of Little Insurgent commem...   \n",
       "3444  In the past, architects, interior designers, e...   \n",
       "116      How many total touchdowns did Cam Newton have?   \n",
       "36                How many fumbles did Von Miller have?   \n",
       "\n",
       "                                      GPP.NS.Prediction  GPP.BS.SacreBLEU  \\\n",
       "4530            What does Goldbach's conjecture assert?         72.003913   \n",
       "507     What was the name of the child in the memorial?         72.597953   \n",
       "3444  What was the past trend in integrating the dif...         65.759095   \n",
       "116      What was the career-high for total touchdowns?         75.062385   \n",
       "36                    What was Von Miller's first sack?         70.710678   \n",
       "\n",
       "      GPP.NS.SacreBLEU  \n",
       "4530          0.792320  \n",
       "507           4.932352  \n",
       "3444          1.040711  \n",
       "116          10.786826  \n",
       "36            6.770186  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where GPP (Beam) performs much better than GPP (Nucleus), but isn't perfect.\n",
    "df[df['GPP.BS.SacreBLEU'] < 80].sort_values(by = 'GPP.Beam.vs.NS.SacreBLEU', ascending = False)[['Reference','Answer','GPP.BS.Prediction','GPP.NS.Prediction','GPP.BS.SacreBLEU','GPP.NS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "facff2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>GPP.BS.Prediction</th>\n",
       "      <th>GPP.NS.Prediction</th>\n",
       "      <th>GPP.BS.SacreBLEU</th>\n",
       "      <th>GPP.NS.SacreBLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>When did ABC's New York flagship stations chan...</td>\n",
       "      <td>May 1, 1953</td>\n",
       "      <td>On what date did ABC change their callsigns to...</td>\n",
       "      <td>When did ABC's New York flagship stations chan...</td>\n",
       "      <td>6.250382</td>\n",
       "      <td>72.925717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>Which material is the Gloucester Candlestick m...</td>\n",
       "      <td>gilt bronze</td>\n",
       "      <td>What is one of the rarest items in the collect...</td>\n",
       "      <td>What type of material is the Gloucester Candle...</td>\n",
       "      <td>4.932352</td>\n",
       "      <td>67.865027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>In what year did Harvard President Joseph Will...</td>\n",
       "      <td>1804</td>\n",
       "      <td>When did the Hollis Professor of Divinity Davi...</td>\n",
       "      <td>In what year did Harvard President Willard die?</td>\n",
       "      <td>8.295194</td>\n",
       "      <td>66.904844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>What did Luther think was the only source of k...</td>\n",
       "      <td>Bible</td>\n",
       "      <td>What did Luther teach that salvation and etern...</td>\n",
       "      <td>What was the only source of knowledge of God?</td>\n",
       "      <td>9.782376</td>\n",
       "      <td>66.940483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>When was Zhu Shijie born?</td>\n",
       "      <td>1249</td>\n",
       "      <td>When did Zhu Shijie solve simultaneous equatio...</td>\n",
       "      <td>What year was Zhu Shijie born?</td>\n",
       "      <td>7.347053</td>\n",
       "      <td>61.478815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference       Answer  \\\n",
       "2880  When did ABC's New York flagship stations chan...  May 1, 1953   \n",
       "2770  Which material is the Gloucester Candlestick m...  gilt bronze   \n",
       "3541  In what year did Harvard President Joseph Will...         1804   \n",
       "1068  What did Luther think was the only source of k...        Bible   \n",
       "4107                          When was Zhu Shijie born?         1249   \n",
       "\n",
       "                                      GPP.BS.Prediction  \\\n",
       "2880  On what date did ABC change their callsigns to...   \n",
       "2770  What is one of the rarest items in the collect...   \n",
       "3541  When did the Hollis Professor of Divinity Davi...   \n",
       "1068  What did Luther teach that salvation and etern...   \n",
       "4107  When did Zhu Shijie solve simultaneous equatio...   \n",
       "\n",
       "                                      GPP.NS.Prediction  GPP.BS.SacreBLEU  \\\n",
       "2880  When did ABC's New York flagship stations chan...          6.250382   \n",
       "2770  What type of material is the Gloucester Candle...          4.932352   \n",
       "3541    In what year did Harvard President Willard die?          8.295194   \n",
       "1068      What was the only source of knowledge of God?          9.782376   \n",
       "4107                     What year was Zhu Shijie born?          7.347053   \n",
       "\n",
       "      GPP.NS.SacreBLEU  \n",
       "2880         72.925717  \n",
       "2770         67.865027  \n",
       "3541         66.904844  \n",
       "1068         66.940483  \n",
       "4107         61.478815  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where GPP (Nucleus) performs much better than GPP (Beam), but isn't perfect.\n",
    "df[df['GPP.NS.SacreBLEU'] < 80].sort_values(by = 'GPP.Beam.vs.NS.SacreBLEU', ascending = True)[['Reference','Answer','GPP.BS.Prediction','GPP.NS.Prediction','GPP.BS.SacreBLEU','GPP.NS.SacreBLEU']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c414b88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Context</th>\n",
       "      <th>Base.BS.Prediction</th>\n",
       "      <th>Base.NS.Prediction</th>\n",
       "      <th>GPP.BS.Prediction</th>\n",
       "      <th>GPP.NS.Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>What is the oldest exhibition site in Warsaw?</td>\n",
       "      <td>Zachęta National Gallery of Art</td>\n",
       "      <td>The 17th century Royal Ujazdów Castle currentl...</td>\n",
       "      <td>What is the oldest exhibition site in Warsaw?</td>\n",
       "      <td>What is the oldest exhibition site in Warsaw?</td>\n",
       "      <td>What is the oldest exhibition site in Warsaw?</td>\n",
       "      <td>What is the oldest exhibition site in Warsaw?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>Who was the first Warsaw President?</td>\n",
       "      <td>Jan Andrzej Menich</td>\n",
       "      <td>The mayor of Warsaw is called President. Gener...</td>\n",
       "      <td>Who was the first Warsaw President?</td>\n",
       "      <td>Who was the first Warsaw President?</td>\n",
       "      <td>Who was the first Warsaw President?</td>\n",
       "      <td>Who was the first Warsaw President?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>What did Tesla Electric Light &amp; Manufacturing do?</td>\n",
       "      <td>installed electrical arc light based illuminat...</td>\n",
       "      <td>After leaving Edison's company Tesla partnered...</td>\n",
       "      <td>What did Tesla Electric Light &amp; Manufacturing do?</td>\n",
       "      <td>What did Tesla Electric Light &amp; Manufacturing do?</td>\n",
       "      <td>What did Tesla Electric Light &amp; Manufacturing do?</td>\n",
       "      <td>What did Tesla Electric Light &amp; Manufacturing do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>How tall was Tesla?</td>\n",
       "      <td>6 feet 2 inches</td>\n",
       "      <td>Tesla was 6 feet 2 inches (1.88 m) tall and we...</td>\n",
       "      <td>How tall was Tesla?</td>\n",
       "      <td>How tall was Tesla?</td>\n",
       "      <td>How tall was Tesla?</td>\n",
       "      <td>How tall was Tesla?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>What is the central business district of San D...</td>\n",
       "      <td>Downtown San Diego</td>\n",
       "      <td>Downtown San Diego is the central business dis...</td>\n",
       "      <td>What is the central business district of San D...</td>\n",
       "      <td>What is the central business district of San D...</td>\n",
       "      <td>What is the central business district of San D...</td>\n",
       "      <td>What is the central business district of San D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reference  \\\n",
       "448       What is the oldest exhibition site in Warsaw?   \n",
       "519                 Who was the first Warsaw President?   \n",
       "658   What did Tesla Electric Light & Manufacturing do?   \n",
       "800                                 How tall was Tesla?   \n",
       "1373  What is the central business district of San D...   \n",
       "\n",
       "                                                 Answer  \\\n",
       "448                     Zachęta National Gallery of Art   \n",
       "519                                  Jan Andrzej Menich   \n",
       "658   installed electrical arc light based illuminat...   \n",
       "800                                     6 feet 2 inches   \n",
       "1373                                 Downtown San Diego   \n",
       "\n",
       "                                                Context  \\\n",
       "448   The 17th century Royal Ujazdów Castle currentl...   \n",
       "519   The mayor of Warsaw is called President. Gener...   \n",
       "658   After leaving Edison's company Tesla partnered...   \n",
       "800   Tesla was 6 feet 2 inches (1.88 m) tall and we...   \n",
       "1373  Downtown San Diego is the central business dis...   \n",
       "\n",
       "                                     Base.BS.Prediction  \\\n",
       "448       What is the oldest exhibition site in Warsaw?   \n",
       "519                 Who was the first Warsaw President?   \n",
       "658   What did Tesla Electric Light & Manufacturing do?   \n",
       "800                                 How tall was Tesla?   \n",
       "1373  What is the central business district of San D...   \n",
       "\n",
       "                                     Base.NS.Prediction  \\\n",
       "448       What is the oldest exhibition site in Warsaw?   \n",
       "519                 Who was the first Warsaw President?   \n",
       "658   What did Tesla Electric Light & Manufacturing do?   \n",
       "800                                 How tall was Tesla?   \n",
       "1373  What is the central business district of San D...   \n",
       "\n",
       "                                      GPP.BS.Prediction  \\\n",
       "448       What is the oldest exhibition site in Warsaw?   \n",
       "519                 Who was the first Warsaw President?   \n",
       "658   What did Tesla Electric Light & Manufacturing do?   \n",
       "800                                 How tall was Tesla?   \n",
       "1373  What is the central business district of San D...   \n",
       "\n",
       "                                      GPP.NS.Prediction  \n",
       "448       What is the oldest exhibition site in Warsaw?  \n",
       "519                 Who was the first Warsaw President?  \n",
       "658   What did Tesla Electric Light & Manufacturing do?  \n",
       "800                                 How tall was Tesla?  \n",
       "1373  What is the central business district of San D...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where all models performed well.\n",
    "threshold = 95\n",
    "df[(df['Base.BS.SacreBLEU'] > threshold) & (df['Base.NS.SacreBLEU'] > threshold) & (df['GPP.BS.SacreBLEU'] > threshold) & (df['GPP.NS.SacreBLEU'] > threshold)][['Reference','Answer','Context','Base.BS.Prediction','Base.NS.Prediction','GPP.BS.Prediction','GPP.NS.Prediction']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0363779a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Context</th>\n",
       "      <th>Base.BS.Prediction</th>\n",
       "      <th>Base.NS.Prediction</th>\n",
       "      <th>GPP.BS.Prediction</th>\n",
       "      <th>GPP.NS.Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Which Carolina Panthers team member was picked...</td>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>The Panthers finished the regular season with ...</td>\n",
       "      <td>Who was named the NFL Most Valuable Player?</td>\n",
       "      <td>Who was named the NFL Most Valuable Player?</td>\n",
       "      <td>Who was named the NFL Most Valuable Player?</td>\n",
       "      <td>What quarterback was named the NFL Most Valuab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Who is the Commissioner of the National Footba...</td>\n",
       "      <td>Roger Goodell</td>\n",
       "      <td>In early 2012, NFL Commissioner Roger Goodell ...</td>\n",
       "      <td>Who stated that the 50th Super Bowl would be a...</td>\n",
       "      <td>Who stated that the 50th Super Bowl would be \"...</td>\n",
       "      <td>Who stated that the 50th Super Bowl was \"spect...</td>\n",
       "      <td>Who stated that the 50th Super Bowl would be \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>What venue in Miami was a candidate for the si...</td>\n",
       "      <td>Sun Life Stadium</td>\n",
       "      <td>The league eventually narrowed the bids to thr...</td>\n",
       "      <td>What stadium did Miami have?</td>\n",
       "      <td>What stadium was Miami's bid for?</td>\n",
       "      <td>What was the name of Miami's Sun Life Stadium?</td>\n",
       "      <td>What stadium was the league's bid to the three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>One of the sites, Merceds-Benz Superdome, is l...</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>The league eventually narrowed the bids to thr...</td>\n",
       "      <td>Where was the Mercedes-Benz Superdome located?</td>\n",
       "      <td>Where was the Mercedes-Benz Superdome located?</td>\n",
       "      <td>Where did the league narrow its bids to three ...</td>\n",
       "      <td>What city did the league narrow its bids to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Who decided not to approve paying for renovati...</td>\n",
       "      <td>Florida legislature</td>\n",
       "      <td>The league announced on October 16, 2012, that...</td>\n",
       "      <td>Who refused to approve the funding plan to pay...</td>\n",
       "      <td>Who refused to approve the funding plan for th...</td>\n",
       "      <td>What did the Florida legislature refuse to app...</td>\n",
       "      <td>What did the Florida legislature refuse to app...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Reference               Answer  \\\n",
       "26  Which Carolina Panthers team member was picked...           Cam Newton   \n",
       "53  Who is the Commissioner of the National Footba...        Roger Goodell   \n",
       "57  What venue in Miami was a candidate for the si...     Sun Life Stadium   \n",
       "61  One of the sites, Merceds-Benz Superdome, is l...          New Orleans   \n",
       "77  Who decided not to approve paying for renovati...  Florida legislature   \n",
       "\n",
       "                                              Context  \\\n",
       "26  The Panthers finished the regular season with ...   \n",
       "53  In early 2012, NFL Commissioner Roger Goodell ...   \n",
       "57  The league eventually narrowed the bids to thr...   \n",
       "61  The league eventually narrowed the bids to thr...   \n",
       "77  The league announced on October 16, 2012, that...   \n",
       "\n",
       "                                   Base.BS.Prediction  \\\n",
       "26        Who was named the NFL Most Valuable Player?   \n",
       "53  Who stated that the 50th Super Bowl would be a...   \n",
       "57                       What stadium did Miami have?   \n",
       "61     Where was the Mercedes-Benz Superdome located?   \n",
       "77  Who refused to approve the funding plan to pay...   \n",
       "\n",
       "                                   Base.NS.Prediction  \\\n",
       "26        Who was named the NFL Most Valuable Player?   \n",
       "53  Who stated that the 50th Super Bowl would be \"...   \n",
       "57                  What stadium was Miami's bid for?   \n",
       "61     Where was the Mercedes-Benz Superdome located?   \n",
       "77  Who refused to approve the funding plan for th...   \n",
       "\n",
       "                                    GPP.BS.Prediction  \\\n",
       "26        Who was named the NFL Most Valuable Player?   \n",
       "53  Who stated that the 50th Super Bowl was \"spect...   \n",
       "57     What was the name of Miami's Sun Life Stadium?   \n",
       "61  Where did the league narrow its bids to three ...   \n",
       "77  What did the Florida legislature refuse to app...   \n",
       "\n",
       "                                    GPP.NS.Prediction  \n",
       "26  What quarterback was named the NFL Most Valuab...  \n",
       "53  Who stated that the 50th Super Bowl would be \"...  \n",
       "57  What stadium was the league's bid to the three...  \n",
       "61  What city did the league narrow its bids to th...  \n",
       "77  What did the Florida legislature refuse to app...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show instances where no models performed well.\n",
    "threshold = 5\n",
    "df[(df['Base.BS.SacreBLEU'] < threshold) & (df['Base.NS.SacreBLEU'] < threshold) & (df['GPP.BS.SacreBLEU'] < threshold) & (df['GPP.NS.SacreBLEU'] < threshold)][['Reference','Answer','Context','Base.BS.Prediction','Base.NS.Prediction','GPP.BS.Prediction','GPP.NS.Prediction']].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
