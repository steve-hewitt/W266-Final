{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e73d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import block\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from packaging import version\n",
    "\n",
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "from datasets import Dataset\n",
    "\n",
    "import sacrebleu\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ee1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions from GP-VAE implementation\n",
    "\n",
    "# Specific to dataset.\n",
    "def construct_input_for_batch(tokenizer, batch, args):\n",
    "    \"\"\"\n",
    "    Function that takes a batch from a dataset and constructs the corresponding \n",
    "    input string.\n",
    "    \"\"\"\n",
    "    source, target = [], []\n",
    "    for inp, out in zip(batch['source'], batch['target']):\n",
    "        source.append(inp.strip())\n",
    "        target.append(out.strip())\n",
    "    if batch['id'][0] == 0:\n",
    "        print(source[0])\n",
    "        print(target[0])\n",
    "        print()\n",
    "    return source, target\n",
    "\n",
    "def make_batch_inputs(batch, tokenizer, args, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Function that takes a batch from a dataset and transforms it \n",
    "  \"\"\"\n",
    "  # Concatenate the concept names for each example in the batch.\n",
    "  input_lists, _ = construct_input_for_batch(tokenizer, batch, args)\n",
    "  # Use the model's tokenizer to create the batch input_ids.\n",
    "  batch_features = tokenizer(input_lists, padding=True, return_tensors='pt')\n",
    "  # Move all inputs to the device.\n",
    "  batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
    "  return batch_features\n",
    "\n",
    "def make_batch_data(batch, tokenizer, args, device='cuda:0'):\n",
    "  \"\"\"\n",
    "  Function that takes a batch from a dataset and transforms it \n",
    "  \"\"\"\n",
    "  # Concatenate the concept names for each example in the batch.\n",
    "  input_lists, label_list = construct_input_for_batch(tokenizer, batch, args)\n",
    "  # Use the model's tokenizer to create the batch input_ids.\n",
    "  batch_features = tokenizer(input_lists, padding=True, return_tensors='pt')\n",
    "  batch_labels = tokenizer(label_list, padding=True, return_tensors='pt')\n",
    "  # Move all inputs to the device.\n",
    "  batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
    "  batch_labels = dict([(k, v.to(device)) for k, v in batch_labels.items()])\n",
    "  return batch_features, batch_labels\n",
    "\n",
    "def batch_tokenize(dataset_batch, tokenizer, args):\n",
    "  \"\"\"\n",
    "  Reuse the function defined above to construct the batch (source, target) and \n",
    "  run them through the tokenizer.\n",
    "  \"\"\"\n",
    "  source, target = construct_input_for_batch(tokenizer, dataset_batch, args)\n",
    "  res = {\n",
    "          \"input_ids\": tokenizer(\n",
    "              source,\n",
    "              padding='max_length', \n",
    "              truncation=True,\n",
    "              max_length=args.encoder_max_length\n",
    "          )[\"input_ids\"],\n",
    "          \"labels\": tokenizer(\n",
    "              target,\n",
    "              padding='max_length', \n",
    "              truncation=True,\n",
    "              max_length=args.decoder_max_length\n",
    "          )[\"input_ids\"],\n",
    "  }\n",
    "  return res\n",
    "\n",
    "def batchify_data(df, tokenizer, args):\n",
    "  dataset = Dataset.from_pandas(df)\n",
    "  data_tokenized = dataset.map(\n",
    "    lambda batch: batch_tokenize(batch, tokenizer, args),\n",
    "    batched=True\n",
    "  )\n",
    "  return data_tokenized\n",
    "\n",
    "def compute_loss(batch, model, tokenizer, args):\n",
    "  batch_feature, batch_label = make_batch_data(batch, tokenizer, args)\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids=batch_feature['input_ids'],\n",
    "                    labels=batch_label['input_ids'])\n",
    "    eval_loss = outputs.loss.item()\n",
    "  return [eval_loss] \n",
    "\n",
    "def test_ppl(val_df, model, tokenizer, args):\n",
    "  loss_dict = Dataset.from_pandas(val_df).map(\n",
    "    lambda batch: {'loss': compute_loss(batch, model, tokenizer, args)},\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "  )\n",
    "  \n",
    "  eval_loss = 0.\n",
    "  nb_eval_steps = 0\n",
    "  for item in list(loss_dict):\n",
    "      eval_loss += item['loss']\n",
    "      nb_eval_steps += 1\n",
    "  eval_loss = eval_loss / nb_eval_steps\n",
    "  ppl = torch.exp(torch.tensor(eval_loss))\n",
    "  return ppl.item()\n",
    "\n",
    "def prepare_eval(output_list):\n",
    "    ref_list, pred_list = [], []\n",
    "    for item in output_list:\n",
    "        pred_list.append({\"generated\": item['generated']})\n",
    "        ref_list.append({\"target\": [item['target']]})\n",
    "    return ref_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defb4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing dataset constructing function from utilities with a custom one.\n",
    "def parse_data(t_split='train'):\n",
    "\n",
    "  # Split handling - validation set further split into 50% dev/test.\n",
    "  if t_split == 'train':\n",
    "    df = pd.DataFrame(load_dataset('squad')['train'])\n",
    "  elif t_split in ['val','test']:\n",
    "    vt_df = pd.DataFrame(load_dataset('squad')['validation'])\n",
    "    df_val = vt_df.sample(frac=0.5,random_state=266)\n",
    "    if t_split == 'test':\n",
    "      df_test = vt_df.drop(df_val.index)\n",
    "      df = df_test\n",
    "    else:\n",
    "      df = df_val\n",
    "  else:\n",
    "    raise Exception(\"Invalid choice of dataset split.\")\n",
    "  \n",
    "\n",
    "  df['answer_text'] = df['answers'].apply(lambda x: x['text'][0])\n",
    "  df['source'] = 'answer: ' + df['answer_text'] + ' context: ' + df['context'] + '</s>'\n",
    "  df['target'] = df['question']\n",
    "\n",
    "  return df                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce922db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary functions from GP-VAE implementation.\n",
    "\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transformers.file_utils import is_apex_available\n",
    "\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "def init_linear_wt(linear):\n",
    "    linear.weight.data.normal_(std=1e-4)\n",
    "    if linear.bias is not None:\n",
    "        linear.bias.data.normal_(std=1e-4)\n",
    "\n",
    "\n",
    "class Seq2SeqModel(T5ForConditionalGeneration):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        self.mean = nn.Linear(config.d_model, config.d_model)\n",
    "        init_linear_wt(self.mean)\n",
    "        self.logvar = nn.Linear(config.d_model, config.d_model)\n",
    "        init_linear_wt(self.logvar)\n",
    "        self.latent2hidden = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        init_linear_wt(self.latent2hidden)\n",
    "        self.hidden2latent = nn.Linear(config.d_model, config.d_model)\n",
    "        init_linear_wt(self.hidden2latent)\n",
    "\n",
    "    def kernel_func(self, x, y):\n",
    "        \"\"\"\n",
    "        x, y - B x 2H\n",
    "        \"\"\"\n",
    "        cov_xy = self.kernel_v * torch.exp(-0.5 * torch.sum(torch.pow((x - y) / self.kernel_r, 2), dim=1))\n",
    "        return cov_xy\n",
    "\n",
    "    def prior(self, hidden_states):\n",
    "        \"\"\"\n",
    "        GP prior p(z|x) = N(mu(x), K(x, x'))\n",
    "        \n",
    "        enc_outputs - B x L x 2H\n",
    "        \"\"\"\n",
    "        b, l, h = list(hidden_states.size())\n",
    "        mean = hidden_states.sum(dim=2)  # B x L\n",
    "        mean = self.hidden2latent(hidden_states) # B x L x K\n",
    "        var = torch.zeros((b, l, l), requires_grad=False).cuda()  # B x L x L\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                var[:, i, j] = self.kernel_func(hidden_states[:, i, :], hidden_states[:, j, :])\n",
    "        return mean, var\n",
    "\n",
    "    def posterior(self, hidden_states):\n",
    "        \"\"\"\n",
    "        variational posterior q(z|x) = N(mu(x), f(x))\n",
    "        \n",
    "        mean, logvar - B x L x K\n",
    "        \"\"\"\n",
    "        mean = self.mean(hidden_states)  # B x L x K\n",
    "        logvar = self.logvar(hidden_states)  # B x L x K\n",
    "        mean = mean.sum(dim=2)  # B x L\n",
    "        x_var = torch.exp(logvar).sum(dim=2)  # B x L\n",
    "        var_batch = []\n",
    "        for b in range(mean.size(0)):\n",
    "            identity_matrix = torch.eye(x_var.size(1)).cuda()\n",
    "            var_batch.append(x_var[b] * identity_matrix)\n",
    "        var = torch.stack(var_batch, dim=0)  # B x L x L\n",
    "        return mean, var\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() * self.scaler\n",
    "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def compute_kld(self, p_mean, p_var, q_mean, q_var):\n",
    "        k = p_var.size(1)\n",
    "        log_det = torch.logdet(p_var) - torch.logdet(q_var)\n",
    "        if torch.isnan(log_det).int().sum() > 0:\n",
    "            if torch.isnan(q_var).int().sum() > 0:\n",
    "                print('q_var has nan!!!')\n",
    "                print(q_var)\n",
    "        try:\n",
    "            p_var_inv = torch.inverse(p_var)  # B x L x L\n",
    "            trace_batch = torch.matmul(p_var_inv, q_var)  # B x L x L\n",
    "            trace_list = [torch.trace(trace_batch[i]) for i in range(trace_batch.size(0))]\n",
    "            trace = torch.stack(trace_list, dim=0)  # B\n",
    "            mean_diff = p_mean - q_mean.unsqueeze(2)  # B x L x 1\n",
    "           \n",
    "            mean = torch.matmul(torch.matmul(mean_diff.transpose(1, 2), p_var_inv), mean_diff)  # B x K x K\n",
    "\n",
    "            kld = log_det - k + trace + torch.mean(mean, dim=(1,2))\n",
    "            kld = 0.5 * kld  # B\n",
    "        except:\n",
    "            zeros = torch.zeros(p_mean.size(0)).cuda()\n",
    "            kld = zeros\n",
    "            print('zero kld!!!')\n",
    "        return kld.mean()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            decoder_input_ids=None,\n",
    "            decoder_attention_mask=None,\n",
    "            head_mask=None,\n",
    "            decoder_head_mask=None,\n",
    "            encoder_outputs=None,\n",
    "            past_key_values=None,\n",
    "            inputs_embeds=None,\n",
    "            decoder_inputs_embeds=None,\n",
    "            labels=None,\n",
    "            use_cache=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "    ):\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                decoder_head_mask = head_mask\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "        # added z code here\n",
    "        posterior_mean = self.mean(hidden_states)  # B x L x K\n",
    "        posterior_logvar = self.logvar(hidden_states)  # B x L x K\n",
    "\n",
    "        if self.from_mean:\n",
    "            z = posterior_mean\n",
    "        else:\n",
    "            z = self.reparameterize(posterior_mean, posterior_logvar)\n",
    "\n",
    "        input_proj = self.latent2hidden(z)  # B x L x K\n",
    "        hidden_states = hidden_states + input_proj\n",
    "\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # If decoding with past key value states, only the last tokens\n",
    "        # should be given as an input\n",
    "        if past_key_values is not None:\n",
    "            assert labels is None, \"Decoder should not use cached key value states when training.\"\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "            if decoder_inputs_embeds is not None:\n",
    "                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.encoder.first_device)\n",
    "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
    "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
    "\n",
    "        if self.config.tie_word_embeddings:\n",
    "            # Rescale output before projecting on vocab\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "            sequence_output = sequence_output * (self.model_dim ** -0.5)\n",
    "\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            # kl_loss\n",
    "            prior_mean, prior_logvar = self.prior(hidden_states)\n",
    "            posterior_mean, posterior_logvar = self.posterior(hidden_states)\n",
    "            kl_loss = self.compute_kld(prior_mean, prior_logvar, posterior_mean, posterior_logvar)\n",
    "            loss = loss + kl_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self,\n",
    "            input_ids,\n",
    "            past=None,\n",
    "            attention_mask=None,\n",
    "            head_mask=None,\n",
    "            decoder_head_mask=None,\n",
    "            #        cross_attn_head_mask=None,\n",
    "            use_cache=None,\n",
    "            encoder_outputs=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "\n",
    "class Seq2SeqTrainer(Trainer):\n",
    "    \"\"\"Class to finetune a Seq2Seq model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_beams=4,\n",
    "            max_length=32,\n",
    "            *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_beams = num_beams\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        outputs = model(input_ids=inputs['input_ids'],\n",
    "                        # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                        labels=inputs['labels'])\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            return self.label_smoother(outputs, inputs[\"labels\"])\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            return outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        Runs the model to either generate a sequence and/or compute the loss.\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        # Compute loss with labels first.\n",
    "        with torch.no_grad():\n",
    "            if self.args.fp16 and _use_native_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids=inputs['input_ids'],\n",
    "                                    # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                                    labels=inputs['labels'])\n",
    "            else:\n",
    "                outputs = model(input_ids=inputs['input_ids'],\n",
    "                                # decoder_input_ids=inputs['labels'][:,:-1],\n",
    "                                labels=inputs['labels'])\n",
    "            if has_labels:\n",
    "                loss = outputs[0].mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "        # If we're only computing the conditional log-likelihood, return.\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        # Otherwise run model.generate() to get predictions.\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            preds = model.module.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        else:\n",
    "            preds = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                num_beams=self.num_beams,\n",
    "                max_length=self.max_length,\n",
    "            )\n",
    "        if len(preds) == 1:\n",
    "            preds = preds[0]\n",
    "        # Pad predictions if necessary so they can be concatenated across batches.\n",
    "        if preds.shape[-1] < self.max_length:\n",
    "            preds = torch.nn.functional.pad(\n",
    "                preds, (0, self.max_length - preds.shape[-1]),\n",
    "                mode='constant',\n",
    "                value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        # Post-process labels.\n",
    "        if has_labels:\n",
    "            labels = inputs.get('labels')\n",
    "        else:\n",
    "            labels = None\n",
    "        return (loss, preds, labels)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # Load the dataset\n",
    "    trn_df = parse_data('train', args)\n",
    "    val_df = parse_data('val', args)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    ckpt_path = None\n",
    "    if args.task == 'train':\n",
    "        ckpt_path = args.model_name\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        # update timestamp and create new path for ckpt\n",
    "        args.timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    train_data_tokenized = batchify_data(trn_df, tokenizer, args)\n",
    "    valid_data_tokenized = batchify_data(val_df, tokenizer, args)\n",
    "\n",
    "    model = Seq2SeqModel.from_pretrained(ckpt_path)\n",
    "    model = model.to('cuda:0')\n",
    "    model.kernel_v = args.kernel_v\n",
    "    model.kernel_r = args.kernel_r\n",
    "    model.from_mean = args.from_mean\n",
    "    model.scaler = 1.0\n",
    "\n",
    "    # Training Setup\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        #save_steps=120,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=300,\n",
    "        #eval_steps=120,\n",
    "        logging_steps=100,\n",
    "        # optimization args, the trainer uses the Adam optimizer\n",
    "        # and has a linear warmup for the learning rate\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-04,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=0,\n",
    "        lr_scheduler_type='constant',\n",
    "        # misc args\n",
    "        seed=42,\n",
    "        save_total_limit=5,  # limit the total amount of checkpoints\n",
    "        disable_tqdm=False,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,\n",
    "        local_rank=args.local_rank\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        num_beams=args.beam_size,\n",
    "        max_length=args.decoder_max_length,\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_data_tokenized,\n",
    "        eval_dataset=valid_data_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Now that we have the trainer set up, we can finetune.\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def beam_generate_sentences(batch,\n",
    "                            model,\n",
    "                            tokenizer,\n",
    "                            args,\n",
    "                            device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "    # Generate with beam search.\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        num_beams=args.beam_size,\n",
    "        max_length=args.max_generation_length,\n",
    "        num_return_sequences=args.num_return_sequences,\n",
    "    )\n",
    "    # Use model tokenizer to decode to text.\n",
    "    generated_sentences = [\n",
    "        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "        for gen_ids in generated_ids\n",
    "    ]\n",
    "    #print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "def nucleus_search_sentences(batch,\n",
    "                            model,\n",
    "                            tokenizer,\n",
    "                            args,\n",
    "                            device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "    # Generate with beam search.\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=features['input_ids'],\n",
    "        attention_mask=features['attention_mask'],\n",
    "        do_sample=True, \n",
    "        max_length=args.max_generation_length,\n",
    "        top_p=args.top_p, \n",
    "        top_k=args.top_k,\n",
    "        num_return_sequences=args.num_return_sequences\n",
    "    )\n",
    "    # Use model tokenizer to decode to text.\n",
    "    generated_sentences = [\n",
    "        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "        for gen_ids in generated_ids\n",
    "    ]\n",
    "    #print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "\n",
    "def sample_sentences(batch,\n",
    "                     model,\n",
    "                     tokenizer,\n",
    "                     args,\n",
    "                     device='cuda:0'):\n",
    "    # Create batch inputs.\n",
    "    features = make_batch_inputs(\n",
    "        batch=batch,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        device=device)\n",
    "\n",
    "    generated_sentences = []\n",
    "    for i in range(args.num_return_sequences):\n",
    "        # Generate with beam search.\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=features['input_ids'],\n",
    "            attention_mask=features['attention_mask'],\n",
    "            num_beams=args.beam_size,\n",
    "            max_length=args.max_generation_length,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        # Use model tokenizer to decode to text.\n",
    "        generated_sentences += [\n",
    "            tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "            for gen_ids in generated_ids\n",
    "        ]\n",
    "    #print(generated_sentences)\n",
    "    return ['\\t'.join(generated_sentences)]\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    te_df = parse_data('test', args)\n",
    "    print('Data loaded!!!')\n",
    "\n",
    "    # Load the model\n",
    "    if args.timestamp == '0':\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(f\"{args.model_name}\")\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "    if args.timestamp == '0':\n",
    "        model = Seq2SeqModel.from_pretrained(f\"{args.model_name}\")\n",
    "    else:\n",
    "        ckpt_path = f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/checkpoint-{args.ckpt}\"\n",
    "        model = Seq2SeqModel.from_pretrained(ckpt_path)\n",
    "    model = model.to('cuda:0')\n",
    "    model.kernel_v = args.kernel_v\n",
    "    model.kernel_r = args.kernel_r\n",
    "    model.from_mean = args.from_mean\n",
    "    model.scaler = args.scaler\n",
    "\n",
    "    # Make predictions\n",
    "    if args.from_mean:\n",
    "        test_output = Dataset.from_pandas(te_df).map(\n",
    "            lambda batch: {'generated': beam_generate_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "    else:\n",
    "        test_output = Dataset.from_pandas(te_df).map(\n",
    "            lambda batch: {'generated': sample_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "    # prepare evaluation data\n",
    "    ref_list, pred_list = prepare_eval(list(test_output))\n",
    "    reference_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": ref_list,\n",
    "    }\n",
    "    prediction_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": pred_list,\n",
    "    }\n",
    "\n",
    "    if args.timestamp == '0':\n",
    "        os.makedirs(f\"{args.model_name}_{args.dataset}_{args.flag}_{args.timestamp}\")\n",
    "\n",
    "    with open(\n",
    "            f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/refs.json\",\n",
    "            'w') as f:\n",
    "        f.write(json.dumps(reference_dict, indent=2))\n",
    "    if args.from_mean:\n",
    "        with open(\n",
    "                f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/outs_mean.json\",\n",
    "                'w') as f:\n",
    "            f.write(json.dumps(prediction_dict, indent=2))\n",
    "    else:\n",
    "        with open(\n",
    "                f\"{args.model_name}_{args.dataset}_{args.flag}_{args.kernel_v}_{args.kernel_r}_{args.timestamp}/outs.json\",\n",
    "                'w') as f:\n",
    "            f.write(json.dumps(prediction_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8320e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = argparse.ArgumentParser(description='Hyperparams')\n",
    "p.add_argument('-t', '--task', type=str, default=\"train\",\n",
    "                help=\"specify the task to do: (train)ing, ft(finetune), (eval)uation\")\n",
    "p.add_argument('-c', '--ckpt', type=str, default=\"600\",\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-time', '--timestamp', type=str, default='2022-07-19-01-04-50',\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-f', '--flag', type=str, default='gpvae',\n",
    "                help=\"Model checkpoint\")\n",
    "p.add_argument('-d', '--dataset', type=str, default=\"GYAFC/em\",\n",
    "                help=\"specify the dataset: GYAFC/em, GYAFC/fr\")\n",
    "p.add_argument('--model_name', type=str, default=\"t5-base\",\n",
    "                help=\"specify the model name: t5-base, facebook/blenderbot-400M-distill\")\n",
    "p.add_argument('-v', '--kernel_v', type=float, default=64.0,\n",
    "                help=\"Hyper-parameter for prior kernel,  control the signal variance\")\n",
    "p.add_argument('-r', '--kernel_r', type=float, default=0.0001,\n",
    "                help=\"Hyper-parameter for prior kernel.\")\n",
    "p.add_argument('-s', '--scaler', type=float, default=1.0)\n",
    "p.add_argument('--from_mean', action='store_true',\n",
    "                help=\"specify whether sample from mean during generation\")\n",
    "p.add_argument('-bz', '--batch_size', type=int, default=8)\n",
    "p.add_argument('-e', '--epochs', type=int, default=10)\n",
    "p.add_argument('--encoder_max_length', type=int, default=256)\n",
    "p.add_argument('--decoder_max_length', type=int, default=48)\n",
    "p.add_argument('--max_generation_length', type=int, default=96)\n",
    "p.add_argument('--beam_size', type=int, default=5)\n",
    "p.add_argument('--num_return_sequences', type=int, default=5)\n",
    "p.add_argument('--local_rank', type=int, default=-1,\n",
    "                help=\"Multiple GPU training\")\n",
    "args = p.parse_args()\n",
    "\n",
    "## jupyter fix for bad flag\n",
    "#args.flag = 't5base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62d461",
   "metadata": {},
   "source": [
    "### Generate predictions on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f51ce139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33022494b1ab4b64adef6648dd5e269c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d979e41b07a4529a736f74296e9add1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get tokenizer, model, and val set.\n",
    "#ckpt_path = f\"t5-base_GYAFC/em_gpvae_64.0_0.0001_2022-07-12-02-30-44/checkpoint-10800\"\n",
    "#ckpt_path = f\"t5-base_GYAFC/em_t5gpp128enc_64.0_0.0001_2022-07-16-14-35-44/checkpoint-4500\" # Pass A\n",
    "#ckpt_path = f\"t5-base_GYAFC/em_t5gpp128enc_64.0_0.0001_2022-07-17-17-38-30/checkpoint-5100\" # Pass B\n",
    "#ckpt_path = f\"t5-base_GYAFC/em_t5gpp128enc_64.0_0.0001_2022-07-19-01-04-50/checkpoint-600\" # Pass C\n",
    "#ckpt_path = \"t5-base_GYAFC/em_grid_e2_100.0_0.001_2022-07-22-02-22-41/checkpoint-5400\" # Best from grid search\n",
    "#ckpt_path = \"t5-base_GYAFC/em_grid_e3_100.0_0.01_2022-07-22-08-10-31/checkpoint-5400\" # 3rd best from grid search\n",
    "#ckpt_path = \"t5-base_GYAFC/em_grid_e4_100.0_0.1_2022-07-22-13-53-38/checkpoint-5400\" # 2nd best from grid search\n",
    "ckpt_path = \"t5-base_GYAFC/em_t5gpp256enc_64.0_0.0001_2022-07-16-04-29-17/checkpoint-10800\" # GPP with 256-length encoder\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "model = Seq2SeqModel.from_pretrained(ckpt_path)\n",
    "val_df = parse_data('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "204d7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other steps\n",
    "model = model.to('cuda:0')\n",
    "model.kernel_v = args.kernel_v\n",
    "model.kernel_r = args.kernel_r\n",
    "model.from_mean = args.from_mean\n",
    "model.scaler = args.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef2e643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff728eb231c41f7bfa98bfa8eff5fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5285 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "test_output = Dataset.from_pandas(val_df).map(\n",
    "    lambda batch: {'generated': beam_generate_sentences(\n",
    "        batch,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        args,\n",
    "        device='cuda:0')\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# prepare evaluation data\n",
    "ref_list, pred_list = prepare_eval(list(test_output))\n",
    "reference_dict = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": ref_list,\n",
    "}\n",
    "prediction_dict = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": pred_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec31ff1",
   "metadata": {},
   "source": [
    "### Score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94efee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 11.06835262894281,\n",
       " 'counts': [22716, 7218, 3534, 1853],\n",
       " 'totals': [59118, 53833, 48548, 43263],\n",
       " 'precisions': [38.42484522480463,\n",
       "  13.408132558096336,\n",
       "  7.279393589849222,\n",
       "  4.283105656103368],\n",
       " 'bp': 0.9831604147503054,\n",
       " 'sys_len': 59118,\n",
       " 'ref_len': 60122}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate BLEU-4.\n",
    "metric = datasets.load_metric('sacrebleu')\n",
    "fin_targets = [reference_dict['values'][x]['target'] for x in range(0,len(reference_dict['values']))]\n",
    "fin_preds = [prediction_dict['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict['values']))]\n",
    "for model_predictions, gold_references in zip(fin_preds,fin_targets):\n",
    "    metric.add(predictions=model_predictions, references=gold_references)\n",
    "final_score = metric.compute()\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "739d4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans: two-thirds majority\n",
      "tgt: How much of a voting majority must there be to effectively censure the Commission?\n",
      "pred: What is the purpose of the European Parliament?\n",
      "\n",
      "ans: British merchants or fur-traders, Céloron informed them of the French claims on the territory and told them to leave.\n",
      "tgt: How did Celeron handle business on trip?\n",
      "pred: What did Céloron tell British merchants about the French claims on the territory?\n",
      "\n",
      "ans: four years\n",
      "tgt: What is the term of office for each house member?\n",
      "pred: What is the term of office for the Victorian Legislative Council?\n",
      "\n",
      "ans: Saudi\n",
      "tgt: What interpretation of Islam is, for many of the adherents, the \"gold standard\" of their religion?\n",
      "pred: What was the purpose of the Wahhabism?\n",
      "\n",
      "ans: the traditional Mongolian aristocracy\n",
      "tgt: Who did Jamukha support that were not part of Temüjin's power base?\n",
      "pred: What did Jamukha support in the Mongolian aristocracy?\n",
      "\n",
      "ans: Catholicism\n",
      "tgt: What religion did the French spread along with their imperialism? \n",
      "pred: What was the purpose of the new empire?\n",
      "\n",
      "ans: 1978\n",
      "tgt: In what year did Roger Rocka's Dinner Theater & Good Company Players open?\n",
      "pred: What year did Roger Rocka's Dinner Theater & Good Company Players open in the Tower District?\n",
      "\n",
      "ans: Papin\n",
      "tgt: Who conceptualized the piston?\n",
      "pred: What was the name of the engine that was proposed by Thomas Newcomer?\n",
      "\n",
      "ans: Aristotle\n",
      "tgt: Who thought that applied force caused movement of an object regardless of non-zero velocity?\n",
      "pred: What does Aristotle believe is the cause of constant velocity motion across a surface with kinetic friction?\n",
      "\n",
      "ans: explanations are reasonably well supported\n",
      "tgt: How are the explanations supported?\n",
      "pred: What are the reasons for the changes in Amazon rainforest vegetation?\n",
      "\n",
      "ans: Roger Goodell\n",
      "tgt: Who if the commissioner of the NFL?\n",
      "pred: Roger Goodell stated that the 50th Super Bowl would be \"spectacular\" and that the 50th Super Bowl would be \"an important game for us as a league\"\n",
      "\n",
      "ans: SAP Center\n",
      "tgt: What venue hosted Super Bowl Opening Night?\n",
      "pred: What was the name of the event held on February 1, 2016 at SAP Center in San Jose?\n",
      "\n",
      "ans: modern soils\n",
      "tgt:  What modern formations do geologists study?\n",
      "pred: What do geologists study?\n",
      "\n",
      "ans: less willing to travel or relocate\n",
      "tgt: What is attributed to the income inequality in the United States?\n",
      "pred: What is the gender pay gap in Bahrain?\n",
      "\n",
      "ans: Sweden v. Russia and allies\n",
      "tgt: Who fought in the great Northern war?\n",
      "pred: What did Sweden v. Russia and allies do?\n",
      "\n",
      "ans: 17\n",
      "tgt: How many interceptions did manning have in 2015?\n",
      "pred: How many interceptions did Manning have?\n",
      "\n",
      "ans: fixed\n",
      "tgt: In Newton's second law, what are the units of mass and force in relation to microscales?\n",
      "pred: What are the relative units of force and mass called in Newton's Second Law?\n",
      "\n",
      "ans: National Galleries of Scotland\n",
      "tgt: With which museum does the V&A co-owns Canova's The Three Graces?\n",
      "pred: What is the name of the museum that owns the Three Graces?\n",
      "\n",
      "ans: staying with the same group of peers for all classes\n",
      "tgt: In a platoon style teaching, what gives the children security?\n",
      "pred: What is a strong sense of security when students stay with the same group of peers for all classes?\n",
      "\n",
      "ans: about 5 nanometers across\n",
      "tgt: How large are the outer PD ring's filaments?\n",
      "pred: What is the outer plastid-dividing ring?\n",
      "\n",
      "ans: its supporters\n",
      "tgt: The idea that Islam can be apolitical isn't able to be embraced by whom?\n",
      "pred: What does Hayri Abaza believe Islamism is?\n",
      "\n",
      "ans: the Red Army\n",
      "tgt: What army did Warsaw successfully defend itself against?\n",
      "pred: What was the name of the war that took place in Warsaw?\n",
      "\n",
      "ans: lysozyme and phospholipase A2\n",
      "tgt: What enzymes in saliva are antibacterial in nature?\n",
      "pred: What do lysozyme and phospholipase A2 do?\n",
      "\n",
      "ans: theses against Agricola\n",
      "tgt: How did Luther respond to Agricola?\n",
      "pred: How did Luther respond to the criticisms of Agricola and the Antinomians?\n",
      "\n",
      "ans: fucoxanthin dinophyte\n",
      "tgt: What lineage is Karenia in?\n",
      "pred: What was the name of the lineage that lost their original red algal derived chloroplast?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z in range(50,75):\n",
    "    print('ans: ' + str(val_df.iloc[z].answers['text'][0]) + '\\ntgt: ' + str(fin_targets[z][0]) + '\\npred: ' + str(fin_preds[z]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd3c4faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"answer: Newcastle Student Radio context: NE1fm launched on 8 June 2007, the first full-time community radio station in the area. Newcastle Student Radio is run by students from both of the city's universities, broadcasting from Newcastle University's student's union building during term time. Radio Tyneside has been the voluntary hospital radio service for most hospitals across Newcastle and Gateshead since 1951, broadcasting on Hospedia  and online. The city also has a Radio Lollipop station based at the Great North Children's Hospital in the Newcastle Royal Victoria Infirmary.</s>\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.iloc[25].source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad3a96",
   "metadata": {},
   "source": [
    "### Save or Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56ab42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pred/target lists.\n",
    "with open('reference_dict_GPP256b.json', 'w') as fp:\n",
    "    json.dump(reference_dict, fp)\n",
    "with open('prediction_dict_GPP256b.json', 'w') as fp:\n",
    "    json.dump(prediction_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2944e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pred/target lists.\n",
    "with open('reference_dict_GPP256b.json', 'r') as fp:\n",
    "    reference_dict = json.load(fp)\n",
    "with open('prediction_dict_GPP256b.json', 'r') as fp:\n",
    "    prediction_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fe973",
   "metadata": {},
   "source": [
    "### Beam experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ff70c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a91d3abc7f4dcfbed2edd867a8659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "args.beam_size = 10\n",
    "args.num_return_sequences = 10\n",
    "test_output = Dataset.from_pandas(val_df[:50]).map(\n",
    "    lambda batch: {'generated': beam_generate_sentences(\n",
    "        batch,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        args,\n",
    "        device='cuda:0')\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# prepare evaluation data\n",
    "ref_list, pred_list = prepare_eval(list(test_output))\n",
    "reference_dict_test = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": ref_list,\n",
    "}\n",
    "prediction_dict_test = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": pred_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56fc7cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'answer: The UK context: The Social Charter was subsequently adopted in 1989 by 11 of the then 12 member states. The UK refused to sign the Social Charter and was exempt from the legislation covering Social Charter issues unless it agreed to be bound by the legislation. The UK subsequently was the only member state to veto the Social Charter being included as the \"Social Chapter\" of the 1992 Maastricht Treaty - instead, an Agreement on Social Policy was added as a protocol. Again, the UK was exempt from legislation arising from the protocol, unless it agreed to be bound by it. The protocol was to become known as \"Social Chapter\", despite not actually being a chapter of the Maastricht Treaty. To achieve aims of the Agreement on Social Policy the European Union was to \"support and complement\" the policies of member states. The aims of the Agreement on Social Policy are:</s>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.iloc[10].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e9a2744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': ['Which member state declined to sign the Social Charter?']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_dict_test['values'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d22e95d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What was the purpose of the Agreement on Social Policy?',\n",
       " 'What did the UK refuse to sign the Treaty of Maastricht?',\n",
       " 'What did the UK refuse to sign?',\n",
       " 'What was the purpose of the agreement on Social Policy?',\n",
       " 'What was the purpose of the Treaty of Maastricht?',\n",
       " 'What was the name of the agreement that the UK refused to sign?',\n",
       " 'What did the UK refuse to sign the Social Charter?',\n",
       " 'What did the UK refuse to sign the Treaty on Social Policy?',\n",
       " 'What was the purpose of the agreement?',\n",
       " 'What was the purpose of the agreement on social policy?']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict_test['values'][10]['generated'].split('\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a954b",
   "metadata": {},
   "source": [
    "### Generate and save predictions for multiple sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e193095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a939cfa120f443b9eebebd332161d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c58ea5639a74177be0a36d4c6be696c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5285 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2  |  {'score': 10.779983790310986, 'counts': [23188, 7296, 3490, 1794], 'totals': [61179, 55894, 50609, 45324], 'precisions': [37.90189444090292, 13.053279421762623, 6.896006639135332, 3.95816785808843], 'bp': 1.0, 'sys_len': 61179, 'ref_len': 60122}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff71255e5ce14fa7b6cb40d509945d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5285 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3  |  {'score': 8.849195174873957, 'counts': [21805, 6178, 2739, 1303], 'totals': [61173, 55888, 50603, 45318], 'precisions': [35.64481061906397, 11.054251359862583, 5.41272256585578, 2.8752372125866104], 'bp': 1.0, 'sys_len': 61173, 'ref_len': 60122}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2ae1fc1ba94b05b227b18e3d36b2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5285 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4  |  {'score': 10.426047933156887, 'counts': [23008, 7080, 3342, 1713], 'totals': [61260, 55975, 50690, 45405], 'precisions': [37.557949722494286, 12.64850379633765, 6.593016374038272, 3.7727122563594317], 'bp': 1.0, 'sys_len': 61260, 'ref_len': 60122}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "paths = (\"t5-base_GYAFC/em_grid_e2_100.0_0.001_2022-07-22-02-22-41/checkpoint-5400\",\n",
    "\"t5-base_GYAFC/em_grid_e3_100.0_0.01_2022-07-22-08-10-31/checkpoint-5400\",\n",
    "\"t5-base_GYAFC/em_grid_e4_100.0_0.1_2022-07-22-13-53-38/checkpoint-5400\")\n",
    "vs = (100,100,100)\n",
    "rs = (0.001,0.01,0.1)\n",
    "names = ('e2','e3','e4')\n",
    "\n",
    "# Get data\n",
    "val_df = parse_data('val')\n",
    "\n",
    "# Main loop\n",
    "for x in range (3):\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(paths[x])\n",
    "    model = Seq2SeqModel.from_pretrained(paths[x])\n",
    "    model = model.to('cuda:0')\n",
    "    model.kernel_v = vs[x]\n",
    "    model.kernel_r = rs[x]\n",
    "    model.from_mean = args.from_mean\n",
    "    model.scaler = args.scaler\n",
    "    \n",
    "    # Make predictions\n",
    "    if args.from_mean:\n",
    "        test_output = Dataset.from_pandas(val_df).map(\n",
    "            lambda batch: {'generated': beam_generate_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "    else:\n",
    "        test_output = Dataset.from_pandas(val_df).map(\n",
    "            lambda batch: {'generated': sample_sentences(\n",
    "                batch,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                args,\n",
    "                device='cuda:0')\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "    # prepare evaluation data\n",
    "    ref_list, pred_list = prepare_eval(list(test_output))\n",
    "    reference_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": ref_list,\n",
    "    }\n",
    "    prediction_dict = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": pred_list,\n",
    "    }\n",
    "    \n",
    "    # Save pred/target lists.\n",
    "    with open(names[x] + '_reference_dict.json', 'w') as fp:\n",
    "        json.dump(reference_dict, fp)\n",
    "    with open(names[x] + '_prediction_dict.json', 'w') as fp:\n",
    "        json.dump(prediction_dict, fp)\n",
    "        \n",
    "    # Calculate BLEU-4.\n",
    "    metric = datasets.load_metric('sacrebleu')\n",
    "    fin_targets = [reference_dict['values'][x]['target'] for x in range(0,len(reference_dict['values']))]\n",
    "    fin_preds = [prediction_dict['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict['values']))]\n",
    "    for model_predictions, gold_references in zip(fin_preds,fin_targets):\n",
    "        metric.add(predictions=model_predictions, references=gold_references)\n",
    "    final_score = metric.compute()\n",
    "    print(names[x], ' | ', final_score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f8038",
   "metadata": {},
   "source": [
    "### Nucleus search experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d966731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ec2-user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e7c3a999024573a3e07d6f78c32b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273a5fa9cbae4cfaa7aba348d042b70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_path = \"t5-base_GYAFC/em_grid_e2_100.0_0.001_2022-07-22-02-22-41/checkpoint-5400\" # Best from grid search\n",
    "#ckpt_path = \"t5-base_GYAFC/em_grid_e3_100.0_0.01_2022-07-22-08-10-31/checkpoint-5400\" # 3rd best from grid search\n",
    "#ckpt_path = \"t5-base_GYAFC/em_grid_e4_100.0_0.1_2022-07-22-13-53-38/checkpoint-5400\" # 2nd best from grid search\n",
    "\n",
    "# Get model and data.\n",
    "tokenizer = T5TokenizerFast.from_pretrained(ckpt_path)\n",
    "model = Seq2SeqModel.from_pretrained(ckpt_path)\n",
    "val_df = parse_data('val')\n",
    "valid_data_tokenized = batchify_data(val_df, tokenizer, args)\n",
    "\n",
    "# Other steps.\n",
    "model = model.to('cuda:0')\n",
    "model.kernel_v = 100 #args.kernel_v\n",
    "model.kernel_r = 0.001 #args.kernel_r\n",
    "model.from_mean = args.from_mean\n",
    "model.scaler = args.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "914b62a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c2a2b33f1946f9942ceb5ff3ab7f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5285 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "args.top_p = 0.5\n",
    "args.top_k = 5\n",
    "args.num_return_sequences = 10\n",
    "\n",
    "test_output2 = Dataset.from_pandas(val_df).map(\n",
    "    lambda batch: {'generated': nucleus_search_sentences(\n",
    "        batch,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        args,\n",
    "        device='cuda:0')\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# prepare evaluation data\n",
    "ref_list2, pred_list2 = prepare_eval(list(test_output))\n",
    "reference_dict2 = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": ref_list2,\n",
    "}\n",
    "prediction_dict2 = {\n",
    "    \"language\": \"en\",\n",
    "    \"values\": pred_list2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2947509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 8.60465608915507,\n",
       " 'counts': [205, 58, 26, 13],\n",
       " 'totals': [550, 500, 450, 400],\n",
       " 'precisions': [37.27272727272727, 11.6, 5.777777777777778, 3.25],\n",
       " 'bp': 0.9064840734837171,\n",
       " 'sys_len': 550,\n",
       " 'ref_len': 604}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate BLEU-4.\n",
    "metric = datasets.load_metric('sacrebleu')\n",
    "fin_targets = [reference_dict2['values'][x]['target'] for x in range(0,len(reference_dict2['values']))]\n",
    "fin_preds = [prediction_dict2['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict2['values']))]\n",
    "for model_predictions, gold_references in zip(fin_preds,fin_targets):\n",
    "    metric.add(predictions=model_predictions, references=gold_references)\n",
    "final_score = metric.compute()\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ad66b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which member state declined to sign the Social Charter?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_dict_test['values'][10]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "956c9035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What was the purpose of the Agreement on Social Policy?',\n",
       " 'What did the UK refuse to sign the Treaty of Maastricht?',\n",
       " 'What did the UK refuse to sign?',\n",
       " 'What was the purpose of the agreement on Social Policy?',\n",
       " 'What was the purpose of the Treaty of Maastricht?',\n",
       " 'What was the name of the agreement that the UK refused to sign?',\n",
       " 'What did the UK refuse to sign the Social Charter?',\n",
       " 'What did the UK refuse to sign the Treaty on Social Policy?',\n",
       " 'What was the purpose of the agreement?',\n",
       " 'What was the purpose of the agreement on social policy?']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict_test['values'][10]['generated'].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6434a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'answer: The UK context: The Social Charter was subsequently adopted in 1989 by 11 of the then 12 member states. The UK refused to sign the Social Charter and was exempt from the legislation covering Social Charter issues unless it agreed to be bound by the legislation. The UK subsequently was the only member state to veto the Social Charter being included as the \"Social Chapter\" of the 1992 Maastricht Treaty - instead, an Agreement on Social Policy was added as a protocol. Again, the UK was exempt from legislation arising from the protocol, unless it agreed to be bound by it. The protocol was to become known as \"Social Chapter\", despite not actually being a chapter of the Maastricht Treaty. To achieve aims of the Agreement on Social Policy the European Union was to \"support and complement\" the policies of member states. The aims of the Agreement on Social Policy are:</s>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.iloc[10].source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c9f8e",
   "metadata": {},
   "source": [
    "### Nucleus search and top-k combination grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dd5da28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4617647f2ec045228e7bc478f2a8083e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 0   {'score': 9.682015563573358, 'counts': [215, 60, 30, 17], 'totals': [580, 530, 480, 430], 'precisions': [37.06896551724138, 11.320754716981131, 6.25, 3.953488372093023], 'bp': 0.9594651258691633, 'sys_len': 580, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e11532501943c7b7e3aba152971bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 1   {'score': 10.659575850113372, 'counts': [231, 69, 32, 19], 'totals': [583, 533, 483, 433], 'precisions': [39.62264150943396, 12.945590994371482, 6.625258799171843, 4.387990762124711], 'bp': 0.9646204383126421, 'sys_len': 583, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ff36b04839412cb9293dc2eb806895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 2   {'score': 10.583598890779632, 'counts': [232, 67, 32, 19], 'totals': [587, 537, 487, 437], 'precisions': [39.522998296422486, 12.476722532588454, 6.570841889117043, 4.3478260869565215], 'bp': 0.9714545275269202, 'sys_len': 587, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0ca229dc0d4828901560e455c6bd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 3   {'score': 10.231060109731834, 'counts': [226, 62, 31, 19], 'totals': [587, 537, 487, 437], 'precisions': [38.500851788756385, 11.54562383612663, 6.365503080082136, 4.3478260869565215], 'bp': 0.9714545275269202, 'sys_len': 587, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2ea091957441779092f629ac5b534f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 4   {'score': 11.467839300930775, 'counts': [232, 70, 38, 21], 'totals': [581, 531, 481, 431], 'precisions': [39.93115318416523, 13.182674199623353, 7.9002079002079, 4.872389791183295], 'bp': 0.9611864048976011, 'sys_len': 581, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d926c1ac6904da39e3470269f6f4b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 5   {'score': 12.516264948532616, 'counts': [231, 78, 42, 24], 'totals': [565, 515, 465, 415], 'precisions': [40.88495575221239, 15.145631067961165, 9.03225806451613, 5.783132530120482], 'bp': 0.9333019018226916, 'sys_len': 565, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f4cbee7a324409b4dee2bb445b2e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.5   k= 10   {'score': 11.17321161512722, 'counts': [232, 72, 36, 19], 'totals': [550, 500, 450, 400], 'precisions': [42.18181818181818, 14.4, 8.0, 4.75], 'bp': 0.9064840734837171, 'sys_len': 550, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba5e8df3c6545d59a6305b5fa18164d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 0   {'score': 8.275053957051519, 'counts': [208, 55, 25, 12], 'totals': [547, 497, 447, 397], 'precisions': [38.02559414990859, 11.066398390342052, 5.592841163310962, 3.022670025188917], 'bp': 0.9010407875484339, 'sys_len': 547, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9724ef48c0b642d1aa94f27f812838b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 1   {'score': 10.520454824289091, 'counts': [231, 66, 32, 19], 'totals': [592, 542, 492, 442], 'precisions': [39.020270270270274, 12.177121771217712, 6.504065040650406, 4.298642533936651], 'bp': 0.9799337905429827, 'sys_len': 592, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4064aef871004b2ebc47170bcf56d0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 2   {'score': 11.045307256657098, 'counts': [234, 68, 34, 21], 'totals': [601, 551, 501, 451], 'precisions': [38.935108153078204, 12.341197822141561, 6.786427145708583, 4.656319290465632], 'bp': 0.9950207572011531, 'sys_len': 601, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1a7e62c247468d9d09d6abc9455c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 3   {'score': 7.88560232292232, 'counts': [206, 47, 23, 13], 'totals': [580, 530, 480, 430], 'precisions': [35.51724137931034, 8.867924528301886, 4.791666666666667, 3.0232558139534884], 'bp': 0.9594651258691633, 'sys_len': 580, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9b222716f84b238303c5f322791220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 4   {'score': 8.145699845053983, 'counts': [208, 53, 23, 13], 'totals': [580, 530, 480, 430], 'precisions': [35.86206896551724, 10.0, 4.791666666666667, 3.0232558139534884], 'bp': 0.9594651258691633, 'sys_len': 580, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ed9eda235647c5b0c196f3e8c111b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 5   {'score': 10.363614057545016, 'counts': [219, 66, 32, 19], 'totals': [599, 549, 499, 449], 'precisions': [36.56093489148581, 12.021857923497267, 6.412825651302605, 4.23162583518931], 'bp': 0.9916874961114086, 'sys_len': 599, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cd2b017f164a8782c0a976ce7b7223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.6   k= 10   {'score': 9.302219859481168, 'counts': [229, 63, 28, 14], 'totals': [590, 540, 490, 440], 'precisions': [38.813559322033896, 11.666666666666666, 5.714285714285714, 3.1818181818181817], 'bp': 0.9765505011069122, 'sys_len': 590, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d444a4ce436a493ca342c1cc0efbf897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 0   {'score': 8.834492151360038, 'counts': [223, 63, 29, 14], 'totals': [631, 581, 531, 481], 'precisions': [35.34072900158478, 10.843373493975903, 5.4613935969868175, 2.9106029106029108], 'bp': 1.0, 'sys_len': 631, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79162438babf4549a558adceade612fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 1   {'score': 9.537542922165255, 'counts': [223, 60, 27, 17], 'totals': [569, 519, 469, 419], 'precisions': [39.19156414762742, 11.560693641618498, 5.756929637526652, 4.05727923627685], 'bp': 0.9403422036318094, 'sys_len': 569, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f116152a6724d2699fbedd9652e8fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 2   {'score': 10.287260267081377, 'counts': [226, 70, 37, 22], 'totals': [660, 610, 560, 510], 'precisions': [34.24242424242424, 11.475409836065573, 6.607142857142857, 4.313725490196078], 'bp': 1.0, 'sys_len': 660, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ba319f34b74fa1af721cbc51122e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 3   {'score': 9.856916590108682, 'counts': [227, 66, 30, 16], 'totals': [599, 549, 499, 449], 'precisions': [37.89649415692821, 12.021857923497267, 6.012024048096192, 3.56347438752784], 'bp': 0.9916874961114086, 'sys_len': 599, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e6affdd0f2455d84718aeb427a8ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 4   {'score': 9.73307364233474, 'counts': [222, 63, 30, 16], 'totals': [579, 529, 479, 429], 'precisions': [38.3419689119171, 11.909262759924385, 6.263048016701461, 3.7296037296037294], 'bp': 0.9577409995699562, 'sys_len': 579, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f92f1f470a1453d9579441149df6e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 5   {'score': 9.943913649183097, 'counts': [211, 60, 30, 19], 'totals': [561, 511, 461, 411], 'precisions': [37.61140819964349, 11.741682974559687, 6.507592190889371, 4.622871046228711], 'bp': 0.9262150449266929, 'sys_len': 561, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15ee181775546d8a6da4017a9cda1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.7   k= 10   {'score': 9.502209876873211, 'counts': [223, 60, 30, 15], 'totals': [562, 512, 462, 412], 'precisions': [39.679715302491104, 11.71875, 6.4935064935064934, 3.6407766990291264], 'bp': 0.9279911377448861, 'sys_len': 562, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508873d5c7ef4342a538d83f7c38cbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 0   {'score': 6.3622578578000955, 'counts': [184, 44, 17, 9], 'totals': [591, 541, 491, 441], 'precisions': [31.133671742808797, 8.133086876155268, 3.4623217922606924, 2.0408163265306123], 'bp': 0.9782435455144597, 'sys_len': 591, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaac0184ccb41529182b14f1bcc08ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 1   {'score': 11.070068256684538, 'counts': [229, 69, 34, 21], 'totals': [584, 534, 484, 434], 'precisions': [39.21232876712329, 12.92134831460674, 7.024793388429752, 4.838709677419355], 'bp': 0.9663332013188017, 'sys_len': 584, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd3ed5e038248deabb5ceedeecec501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 2   {'score': 9.287065428761597, 'counts': [223, 61, 27, 15], 'totals': [566, 516, 466, 416], 'precisions': [39.399293286219084, 11.821705426356589, 5.793991416309013, 3.605769230769231], 'bp': 0.935066331726045, 'sys_len': 566, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6802e68209f74bd99d832210a49e5f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 3   {'score': 6.712266477247548, 'counts': [204, 46, 20, 9], 'totals': [615, 565, 515, 465], 'precisions': [33.170731707317074, 8.141592920353983, 3.883495145631068, 1.935483870967742], 'bp': 1.0, 'sys_len': 615, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d8311190b4864a0e0ef4ed237d9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 4   {'score': 8.220325651881177, 'counts': [206, 49, 24, 14], 'totals': [570, 520, 470, 420], 'precisions': [36.14035087719298, 9.423076923076923, 5.1063829787234045, 3.3333333333333335], 'bp': 0.9420950352596025, 'sys_len': 570, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4eaa1b082c42c186742f7ceb484d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 5   {'score': 8.132983123213684, 'counts': [219, 55, 25, 12], 'totals': [614, 564, 514, 464], 'precisions': [35.66775244299674, 9.75177304964539, 4.863813229571984, 2.586206896551724], 'bp': 1.0, 'sys_len': 614, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c973df0d94d5198da8c5480adfad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.75   k= 10   {'score': 9.078027407461255, 'counts': [226, 63, 30, 16], 'totals': [641, 591, 541, 491], 'precisions': [35.257410296411855, 10.65989847715736, 5.545286506469501, 3.258655804480652], 'bp': 1.0, 'sys_len': 641, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73119c81df37495ca93cd24aa115b166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 0   {'score': 5.675077275321855, 'counts': [178, 32, 17, 8], 'totals': [577, 527, 477, 427], 'precisions': [30.849220103986134, 6.072106261859583, 3.5639412997903563, 1.873536299765808], 'bp': 0.9542841880738105, 'sys_len': 577, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca98b3143f848a6858ad75a70f7abfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 1   {'score': 10.861793134960227, 'counts': [232, 68, 33, 20], 'totals': [579, 529, 479, 429], 'precisions': [40.06908462867012, 12.854442344045369, 6.8893528183716075, 4.662004662004662], 'bp': 0.9577409995699562, 'sys_len': 579, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3874258caa4025be1019e5dbe57041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 2   {'score': 10.373654673525335, 'counts': [222, 65, 31, 19], 'totals': [552, 502, 452, 402], 'precisions': [40.21739130434783, 12.94820717131474, 6.8584070796460175, 4.72636815920398], 'bp': 0.9100980859234709, 'sys_len': 552, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e24f73ad754494a8662636fbd08eb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 3   {'score': 8.360420010033472, 'counts': [211, 55, 24, 13], 'totals': [567, 517, 467, 417], 'precisions': [37.213403880070544, 10.638297872340425, 5.139186295503212, 3.117505995203837], 'bp': 0.9368278559525164, 'sys_len': 567, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807033ef803048749758991046a9df4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 4   {'score': 8.452380216937575, 'counts': [200, 54, 25, 14], 'totals': [566, 516, 466, 416], 'precisions': [35.3356890459364, 10.465116279069768, 5.364806866952789, 3.3653846153846154], 'bp': 0.935066331726045, 'sys_len': 566, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76959db4ca5f4c0c94ee35eb20b71b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 5   {'score': 8.375117586386075, 'counts': [197, 53, 25, 14], 'totals': [570, 520, 470, 420], 'precisions': [34.56140350877193, 10.192307692307692, 5.319148936170213, 3.3333333333333335], 'bp': 0.9420950352596025, 'sys_len': 570, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefa735ee53c47e1af2e05f39901d057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.8   k= 10   {'score': 8.429844764188978, 'counts': [200, 51, 26, 14], 'totals': [555, 505, 455, 405], 'precisions': [36.03603603603604, 10.099009900990099, 5.714285714285714, 3.45679012345679], 'bp': 0.9154969116385431, 'sys_len': 555, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e199249edb60467d8e25ce62da4cda79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 0   {'score': 4.799927566767934, 'counts': [186, 32, 11, 6], 'totals': [565, 515, 465, 415], 'precisions': [32.92035398230089, 6.213592233009709, 2.3655913978494625, 1.4457831325301205], 'bp': 0.9333019018226916, 'sys_len': 565, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6ab9c311c54765b54e474d0ccb4f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 1   {'score': 10.016836485163958, 'counts': [225, 63, 30, 18], 'totals': [597, 547, 497, 447], 'precisions': [37.688442211055275, 11.517367458866545, 6.036217303822937, 4.026845637583893], 'bp': 0.9883431802324741, 'sys_len': 597, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce225e47fe194f2f9de777a2a5aca0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 2   {'score': 10.57200460890163, 'counts': [228, 64, 34, 19], 'totals': [589, 539, 489, 439], 'precisions': [38.70967741935484, 11.873840445269016, 6.952965235173824, 4.328018223234624], 'bp': 0.9748546529064186, 'sys_len': 589, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b1880e424949b981e6570f843e0ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 3   {'score': 9.653260347824855, 'counts': [213, 60, 30, 17], 'totals': [583, 533, 483, 433], 'precisions': [36.53516295025729, 11.25703564727955, 6.211180124223603, 3.9260969976905313], 'bp': 0.9646204383126421, 'sys_len': 583, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0227dd62789844738f9ab24aea80bb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 4   {'score': 8.201945324032963, 'counts': [211, 55, 24, 12], 'totals': [562, 512, 462, 412], 'precisions': [37.544483985765126, 10.7421875, 5.194805194805195, 2.912621359223301], 'bp': 0.9279911377448861, 'sys_len': 562, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66b19f015da4ea28dda6ac9f37069eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 5   {'score': 4.223408246708841, 'counts': [191, 34, 9, 4], 'totals': [554, 504, 454, 404], 'precisions': [34.47653429602888, 6.746031746031746, 1.9823788546255507, 0.9900990099009901], 'bp': 0.9137002571118673, 'sys_len': 554, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807d40a77bc1426b8ca3536f1d47dc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.85   k= 10   {'score': 5.692763790904477, 'counts': [199, 40, 14, 7], 'totals': [570, 520, 470, 420], 'precisions': [34.91228070175438, 7.6923076923076925, 2.978723404255319, 1.6666666666666667], 'bp': 0.9420950352596025, 'sys_len': 570, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f74119bec7427b960b94918859b924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 0   {'score': 5.330926230031674, 'counts': [192, 42, 15, 8], 'totals': [666, 616, 566, 516], 'precisions': [28.82882882882883, 6.818181818181818, 2.65017667844523, 1.550387596899225], 'bp': 1.0, 'sys_len': 666, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ea3d3958394b1287ca765b79077925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 1   {'score': 11.160581030284982, 'counts': [233, 70, 34, 21], 'totals': [583, 533, 483, 433], 'precisions': [39.96569468267582, 13.133208255159476, 7.0393374741200825, 4.849884526558891], 'bp': 0.9646204383126421, 'sys_len': 583, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79df4901c03d4983afb568300fa58a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 2   {'score': 8.648394348903828, 'counts': [214, 59, 25, 13], 'totals': [551, 501, 451, 401], 'precisions': [38.83847549909256, 11.776447105788423, 5.5432372505543235, 3.2418952618453867], 'bp': 0.9082925617279349, 'sys_len': 551, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d97bd337a240c3bdd9ea0b0b6e3068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 3   {'score': 8.447435192319988, 'counts': [203, 51, 26, 14], 'totals': [565, 515, 465, 415], 'precisions': [35.92920353982301, 9.902912621359222, 5.591397849462366, 3.3734939759036147], 'bp': 0.9333019018226916, 'sys_len': 565, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f1701be28f48ca9e806b4c31d00683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 4   {'score': 5.999911215367029, 'counts': [205, 45, 15, 7], 'totals': [578, 528, 478, 428], 'precisions': [35.46712802768166, 8.522727272727273, 3.1380753138075312, 1.6355140186915889], 'bp': 0.9560140217249987, 'sys_len': 578, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a4806d1f2744f188233bd794799138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 5   {'score': 10.68162761074252, 'counts': [213, 65, 35, 20], 'totals': [573, 523, 473, 423], 'precisions': [37.17277486910995, 12.4282982791587, 7.399577167019028, 4.7281323877068555], 'bp': 0.9473362107231725, 'sys_len': 573, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036d373548654229a9b9c10c2fdf2ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.9   k= 10   {'score': 4.943196367765321, 'counts': [199, 44, 13, 4], 'totals': [600, 550, 500, 450], 'precisions': [33.166666666666664, 8.0, 2.6, 0.8888888888888888], 'bp': 0.9933555062550344, 'sys_len': 600, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4a21e94d2d4d76ac31c44949c3b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 0   {'score': 6.86278322173984, 'counts': [179, 44, 21, 10], 'totals': [575, 525, 475, 425], 'precisions': [31.130434782608695, 8.380952380952381, 4.421052631578948, 2.3529411764705883], 'bp': 0.9508159363852959, 'sys_len': 575, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c60d9d8f9a4c03bde2c31fd8f6cb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 1   {'score': 11.098948601268633, 'counts': [231, 69, 34, 21], 'totals': [582, 532, 482, 432], 'precisions': [39.69072164948454, 12.969924812030076, 7.053941908713693, 4.861111111111111], 'bp': 0.9629048409444241, 'sys_len': 582, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982bb3f5b4d645b9bd6ebceef7fa3875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 2   {'score': 8.279659529458689, 'counts': [198, 56, 26, 12], 'totals': [557, 507, 457, 407], 'precisions': [35.5475763016158, 11.045364891518737, 5.689277899343545, 2.9484029484029484], 'bp': 0.9190813775730131, 'sys_len': 557, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1ad09f07bd4c8b8c77be9722621d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 3   {'score': 6.446132588584179, 'counts': [191, 47, 18, 8], 'totals': [580, 530, 480, 430], 'precisions': [32.93103448275862, 8.867924528301886, 3.75, 1.8604651162790697], 'bp': 0.9594651258691633, 'sys_len': 580, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caed1439028d46e8a9a94db4a8245740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 4   {'score': 6.8680633646063445, 'counts': [215, 47, 20, 11], 'totals': [640, 590, 540, 490], 'precisions': [33.59375, 7.966101694915254, 3.7037037037037037, 2.2448979591836733], 'bp': 1.0, 'sys_len': 640, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528ba81ec196478c9937d58eed105e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 5   {'score': 9.582246089214346, 'counts': [193, 52, 30, 21], 'totals': [582, 532, 482, 432], 'precisions': [33.16151202749141, 9.774436090225564, 6.224066390041494, 4.861111111111111], 'bp': 0.9629048409444241, 'sys_len': 582, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f54c941bfa54ba099e3118fc12c81ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.925   k= 10   {'score': 4.5880962268179015, 'counts': [172, 34, 14, 4], 'totals': [563, 513, 463, 413], 'precisions': [30.550621669627, 6.627680311890838, 3.023758099352052, 0.9685230024213075], 'bp': 0.9297643088430873, 'sys_len': 563, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f12ee09e5c466bb0b6689e40d64d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 0   {'score': 6.59726183893042, 'counts': [178, 37, 20, 11], 'totals': [603, 553, 503, 453], 'precisions': [29.519071310116086, 6.6907775768535265, 3.9761431411530817, 2.4282560706401766], 'bp': 0.9983429995509426, 'sys_len': 603, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7625e66105a04dcfa4c101321ec4f64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 1   {'score': 11.007390960980457, 'counts': [229, 67, 34, 21], 'totals': [576, 526, 476, 426], 'precisions': [39.75694444444444, 12.737642585551331, 7.142857142857143, 4.929577464788732], 'bp': 0.9525514943707395, 'sys_len': 576, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e6bbb51d04434ebc29bc01a69540e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 2   {'score': 7.1157927282049664, 'counts': [218, 49, 20, 9], 'totals': [582, 532, 482, 432], 'precisions': [37.45704467353952, 9.210526315789474, 4.149377593360996, 2.0833333333333335], 'bp': 0.9629048409444241, 'sys_len': 582, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a75f55cd99a4301ba5bd00fd0da2567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 3   {'score': 7.473605049321506, 'counts': [210, 54, 21, 10], 'totals': [601, 551, 501, 451], 'precisions': [34.94176372712146, 9.800362976406534, 4.191616766467066, 2.2172949002217295], 'bp': 0.9950207572011531, 'sys_len': 601, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd09eafd838427b869840812afb38e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 4   {'score': 5.36641060058993, 'counts': [191, 35, 13, 7], 'totals': [551, 501, 451, 401], 'precisions': [34.66424682395644, 6.986027944111776, 2.882483370288248, 1.745635910224439], 'bp': 0.9082925617279349, 'sys_len': 551, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66e8cd663bf49b3adaf8d6a57797b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 5   {'score': 9.058336655228615, 'counts': [218, 66, 31, 17], 'totals': [657, 607, 557, 507], 'precisions': [33.181126331811264, 10.873146622734762, 5.565529622980251, 3.353057199211045], 'bp': 1.0, 'sys_len': 657, 'ref_len': 604}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ff8cc44d7c4dbe83b342081284c79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 0.95   k= 10   {'score': 5.6750751394068235, 'counts': [184, 40, 15, 7], 'totals': [574, 524, 474, 424], 'precisions': [32.055749128919864, 7.633587786259542, 3.1645569620253164, 1.650943396226415], 'bp': 0.9490775099024896, 'sys_len': 574, 'ref_len': 604}\n"
     ]
    }
   ],
   "source": [
    "p_list = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.925, 0.95]\n",
    "k_list = [0, 1, 2, 3, 4, 5, 10]\n",
    "pk_list = [(p,k) for p in p_list for k in k_list]\n",
    "\n",
    "for x in range(len(pk_list)):\n",
    "    # Make predictions\n",
    "    args.top_p = pk_list[x][0]\n",
    "    args.top_k = pk_list[x][1]\n",
    "\n",
    "    test_output = Dataset.from_pandas(val_df[:50]).map(\n",
    "        lambda batch: {'generated': nucleus_search_sentences(\n",
    "            batch,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            args,\n",
    "            device='cuda:0')\n",
    "        },\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    # prepare evaluation data\n",
    "    ref_list, pred_list = prepare_eval(list(test_output))\n",
    "    reference_dict_test = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": ref_list,\n",
    "    }\n",
    "    prediction_dict_test = {\n",
    "        \"language\": \"en\",\n",
    "        \"values\": pred_list,\n",
    "    }\n",
    "    \n",
    "    # Calculate BLEU-4.\n",
    "    metric = datasets.load_metric('sacrebleu')\n",
    "    fin_targets = [reference_dict_test['values'][x]['target'] for x in range(0,len(reference_dict['values']))]\n",
    "    fin_preds = [prediction_dict_test['values'][x]['generated'].split('\\t')[0] for x in range(0,len(prediction_dict['values']))]\n",
    "    for model_predictions, gold_references in zip(fin_preds,fin_targets):\n",
    "        metric.add(predictions=model_predictions, references=gold_references)\n",
    "    final_score = metric.compute()\n",
    "    print('p=',args.top_p,' ',\n",
    "          'k=',args.top_k,' ',\n",
    "        final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e9d0e",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96cbf5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "  (mean): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (logvar): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (latent2hidden): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (hidden2latent): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "caa264de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.latent2hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f84cb5",
   "metadata": {},
   "source": [
    "### Test on novel examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3485001c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986884e094e430d823b740c17a6f656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Batman context: The Justice League is made up of many heroes. Superman is an alien with super strength, x-ray vision, and the ability to fly. Batman uses his vast wealth to buy gadgets. Wonder Woman is an Amazon princess with an invisible jet, bulletproof bracelets, and a lasso of truth. The Flash is just really fast.\n",
      "Who is the richest member of the Justice League?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What is the name of the superhero in the Justice League?',\n",
       " \"What is Superman's role in the Justice League?\",\n",
       " \"What is Batman's role in the Justice League?\",\n",
       " 'What is the name of the superhero who has the ability to fly?',\n",
       " \"What is the name of the superhero in Batman's Justice League?\",\n",
       " \"What is Superman's name?\",\n",
       " \"What is Superman's nickname?\",\n",
       " \"What is the name of Batman's favorite superhero?\",\n",
       " \"What is Superman's role in the Justice League called?\",\n",
       " 'What is the name of the superhero who has the power to fly?']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize beam size and return sequences.\n",
    "args.beam_size = 10\n",
    "args.num_return_sequences = 10\n",
    "\n",
    "# Example A - Superhero Question\n",
    "novel_answer = \"Batman\"\n",
    "novel_context = \"The Justice League is made up of many heroes. Superman is an alien with super strength, x-ray vision, and the ability to fly. Batman uses his vast wealth to buy gadgets. Wonder Woman is an Amazon princess with an invisible jet, bulletproof bracelets, and a lasso of truth. The Flash is just really fast.\"\n",
    "novel_df = pd.DataFrame()\n",
    "novel_df['source'] = [\"answer: \" + novel_answer + \" context: \" + novel_context]\n",
    "novel_df['target'] = ['Who is the richest member of the Justice League?']\n",
    "novel_df['id'] = 0\n",
    "\n",
    "\n",
    "novel_example_output = Dataset.from_pandas(novel_df).map(\n",
    "    lambda batch: {'generated': beam_generate_sentences(\n",
    "        batch,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        args,\n",
    "        device='cuda:0')\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "novel_example_output\n",
    "\n",
    "# Prepare evaluation data\n",
    "novel_refs_list, novel_preds_list = prepare_eval(list(novel_example_output))\n",
    "\n",
    "# Show output\n",
    "novel_preds_list[0]['generated'].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cc12dabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca542644f2704ac8ad3c758c2d5a260a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Stochastic gradient descent context: Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in trade for a lower convergence rate.\n",
      "What does SGD stand for?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What does SGD stand for?',\n",
       " 'What is the acronym for SGD?',\n",
       " 'What is an example of a stochastic gradient descent?',\n",
       " 'What is a synonym for SGD?',\n",
       " 'What is SGD?',\n",
       " 'What is SGD also known as?',\n",
       " 'What is the acronym of SGD?',\n",
       " 'What is an example of a stochastic approach to gradient descent?',\n",
       " 'What is the name of the process that optimizes an objective function?',\n",
       " 'What is an example of a stochastic gradient descent technique?']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize beam size and return sequences.\n",
    "args.beam_size = 10\n",
    "args.num_return_sequences = 10\n",
    "\n",
    "# Example B - Machine Learning\n",
    "novel_answer = \"Stochastic gradient descent\"\n",
    "novel_context = \"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in trade for a lower convergence rate.\"\n",
    "novel_df = pd.DataFrame()\n",
    "novel_df['source'] = [\"answer: \" + novel_answer + \" context: \" + novel_context]\n",
    "novel_df['target'] = ['What does SGD stand for?']\n",
    "novel_df['id'] = 0\n",
    "\n",
    "\n",
    "novel_example_output = Dataset.from_pandas(novel_df).map(\n",
    "    lambda batch: {'generated': beam_generate_sentences(\n",
    "        batch,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        args,\n",
    "        device='cuda:0')\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "novel_example_output\n",
    "\n",
    "# Prepare evaluation data\n",
    "novel_refs_list, novel_preds_list = prepare_eval(list(novel_example_output))\n",
    "\n",
    "# Show output\n",
    "novel_preds_list[0]['generated'].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "72d745c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbe32f8c09641f8820d83d2c81c9991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: 1066 context: William I, usually known as William the Conqueror and sometimes William the Bastard, was the first Norman king of England, reigning from 1066 until his death in 1087. A descendant of Rollo, he was Duke of Normandy from 1035 onward. By 1060, following a long struggle to establish his throne, his hold on Normandy was secure. In 1066, following the death of Edward the Confessor, William invaded England, leading an army of Normans to victory over the Anglo-Saxon forces of Harold Godwinson at the Battle of Hastings, and suppressed subsequent English revolts in what has become known as the Norman Conquest. The rest of his life was marked by struggles to consolidate his hold over England and his continental lands, and by difficulties with his eldest son, Robert Curthose.\n",
      "At which battle did William the Conqueror defeat Harold Godwinson?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What year was William the Conqueror king of England?',\n",
       " 'What year was William I king of England?',\n",
       " 'What was the name of the first Norman king of England?',\n",
       " \"What was William the Conqueror's name?\",\n",
       " 'What year did William I reign as king of England?',\n",
       " 'What year was William the Conqueror born?',\n",
       " 'What year was William the Conqueror king of England crowned?',\n",
       " \"What was William the Conqueror's title?\",\n",
       " 'What year was William the Conqueror king of England born?',\n",
       " 'What year was William the Conqueror king?']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize beam size and return sequences.\n",
    "args.beam_size = 10\n",
    "args.num_return_sequences = 10\n",
    "\n",
    "# Example C - World History\n",
    "novel_answer = \"1066\"\n",
    "novel_context = \"William I, usually known as William the Conqueror and sometimes William the Bastard, was the first Norman king of England, reigning from 1066 until his death in 1087. A descendant of Rollo, he was Duke of Normandy from 1035 onward. By 1060, following a long struggle to establish his throne, his hold on Normandy was secure. In 1066, following the death of Edward the Confessor, William invaded England, leading an army of Normans to victory over the Anglo-Saxon forces of Harold Godwinson at the Battle of Hastings, and suppressed subsequent English revolts in what has become known as the Norman Conquest. The rest of his life was marked by struggles to consolidate his hold over England and his continental lands, and by difficulties with his eldest son, Robert Curthose.\"\n",
    "novel_df = pd.DataFrame()\n",
    "novel_df['source'] = [\"answer: \" + novel_answer + \" context: \" + novel_context]\n",
    "novel_df['target'] = ['At which battle did William the Conqueror defeat Harold Godwinson?']\n",
    "novel_df['id'] = 0\n",
    "\n",
    "\n",
    "novel_example_output = Dataset.from_pandas(novel_df).map(\n",
    "    lambda batch: {'generated': beam_generate_sentences(\n",
    "        batch,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        args,\n",
    "        device='cuda:0')\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "novel_example_output\n",
    "\n",
    "# Prepare evaluation data\n",
    "novel_refs_list, novel_preds_list = prepare_eval(list(novel_example_output))\n",
    "\n",
    "# Show output\n",
    "novel_preds_list[0]['generated'].split('\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
